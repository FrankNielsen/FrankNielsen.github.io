<!DOCTYPE html>
<html>
<body>
<h1>Some machine learning tricks</h1>

<em>Because there is nothing more useful than a mathematical trick!</em><BR>
(Click slides to enlarge them)



<ul>


<li>Log-likelihood trick<BR>
<A HREF="Card-LogLikelihoodTrick.png" target="_blank"><IMG SRC="Card-LogLikelihoodTrick.png" width="25%"></A>




<li>Thermodynamic integration and Fisher-Rao geodesics:<BR>
<A HREF="Card-ThermodynamicIntegrationFisherRaoGeodesic.png" target="_blank"><IMG SRC="Card-ThermodynamicIntegrationFisherRaoGeodesic.png" width="25%"></A>



<li>kernel trick<BR> 
     (Euclidean to Hilbert space ML, RKHS)
	 
	 
<li>probability integral transform trick 
    (continuous distribution->uniform)
	
	
<li>Maximum Mean Discrepancy (MMD) trick
    (avoid density estimation)
<li>log-sum-exp (softmax) trick
     (smoothen maximum function)
<li>replica trick
     (thermostatistics/log partition function)
<li>Gumbel-max trick
     (approximate expectation)
<li>Russian Roulette trick
     (unbiased estimator for sums/series)
<li>Pearlmutter’s fast Hessian-vector product
<li>contrastive divergence (graphical models)
<li>majorization minimization trick (EM etc.)


<li>Bayesian conjugacy trick
     (conjugate priors of exponential families)
<li>reparameterization trick
     (backprop in DL through stochastic nodes)
<li>duality tricks
     (dual problem easier, Fenchel, OT, MI, etc.)
<li>Feynman integral trick (Thermodynamic VO)
     (complex integral solves easier)
<li>doubly reparameterized Gradient Estimators 
<li>backpropagation Through Time (BPTT)
     (LSTM, etc.)
<li>Woodbury matrix inversion Lemma 
(natural gradient)

<li>Hutchinson’s stochastic trace trick 
     (randomized linear algebra for large scale ML)
<li>concentration inequalities (theo. bounds) 
<li>Kronecker factorization (natural gradient K-FAC)
<li>Bootstrapping trick





</ul>

<hr>
Last updated by Frank.Nielsen@acm.org, July 2020.
 

</body>
</html>
