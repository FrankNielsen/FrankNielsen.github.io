<!DOCTYPE html>
<html lang="en">
<meta charset="UTF-8">
<title>Minicourse: Introduction to Information Geometry, Recent Advances and Applications</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<link rel="stylesheet" href="">
<style>
</style>
<script src=""></script>


<body>

<h1>Introduction to Information Geometry, Recent Advances and Applications</h1>

Minicourse: Tuesday August 29th 2023.<br>
<A HREF="http://www.baltinmat.com/wisla23/" target="_blank">School and workshop on <style="color:red">Mapping the Interdisciplinary Horizons of AI: 
Risks, Alignment, Functional Programming, Information Geometry and Beyond</A><BR>
Where: <A HREF="http://www.baltinmat.com/" target="_blank">Baltic Institute of Mathematics</A> (hybrid format: Gothenburg, Sweden and online)<BR>
When: August 21st-September 1st 2023   

<p>
Information geometry primarily studies the geometric structures, dissimilarities, and statistical invariance 
of a family of probability distributions called the statistical model. 
A regular parametric statistical model can be geometrically handled as 
a Riemannian manifold equipped with the Fisher metric tensor which induces the Fisher-Rao geodesic distance.
This Riemannian structure on the Fisher-Rao manifold was 
later generalized by a dual structure based on pairs of torsion-free affine connections 
coupled to the Fisher metric: The α-geometry. 
This dual structure casts light on the close interaction between statistical estimators 
in inference  (maximum likelihood) and  parametric statistical models (exponential families obtained from the principle of maximum entropy), 
and brings into play a generalized Pythagorean theorem useful to prove uniqueness of information projections. 
We will illustrate applications of information geometry in statistics, information theory, computer vision and pattern recognition, and learning of neural networks.
The second part of the minicourse will present recent advances in information geometry and its applications.
</p>


<ul>
<li>Part I: <A HREF="https://www.youtube.com/watch?v=w6r_jsEBlgU" target="_blank">Introduction to Information Geometry</A>
<BR>&nbsp;<BR>
<li>Readings:
<ul>
<li><A HREF="https://www.ams.org/journals/notices/202201/rnoti-p36.pdf" target="_blank">
The Many Faces of Information Geometry</A> (AMS Notices 2022)
<li><A HREF="https://www.mdpi.com/1099-4300/22/10/1100" target="_blank">
Elementary introduction to information geometry</A> (Entropy 2020)

</ul>
<BR>&nbsp;<BR>
<li>Part II: <A HREF="" target="_blank">Recent advances and applications of information geometry</A>
Readings:
<BR>&nbsp;<BR>
<ul>
<li>Maximal invariant:
<ul>
<li><A HREF="https://arxiv.org/abs/2101.12459" target="_blank">
On f-Divergences Between Cauchy Distributions</a>, IEEE Transactions on Information Theory, 2023 (arXiv:2101.12459)
<li><A HREF="https://arxiv.org/abs/2204.10952" target="_blank">
A note on the f-divergences between multivariate location-scale families with either
 prescribed scale matrices or location parameters</A>, 
 arXiv:2204.10952 
 
<li><A HREF="https://arxiv.org/abs/2205.13984" target="_blank">
On the f$-divergences between hyperboloid and Poincaré distributions</A>, arxiv:2205.13984 (2022)
</ul>

<li>Exponential families and divergences:
<ul>
<li><A HREF="https://www.mdpi.com/1099-4300/24/3/421" target="_blank">
Statistical Divergences between Densities of Truncated Exponential Families with Nested Supports: 
Duo Bregman and Duo Jensen Divergences</A>, 
Entropy 2022
<li><A HREF="https://www.mdpi.com/1099-4300/24/10/1400" target="_blank">Revisiting
 Chernoff Information with Likelihood Ratio Exponential Families</A>, Entropy 2022
</ul>

<li>Fisher-Rao geometry between multivariate normal distributions, TBA.
</ul>

<BR>&nbsp;<BR>
<li>Some exercises
</ul> 

</body>
</html>