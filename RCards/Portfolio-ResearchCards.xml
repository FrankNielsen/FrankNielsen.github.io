
<Portfolio>


<card>
<visual>
RCard-scaledJensenBregmanDivs.png
</visual>
<description>
Get Bregman divergence and reverse Bregman divergence by taking limits of scaled α-skew Jensen divergences. Scaled α-skew Jensen divergences are defined for **any  α in R\{0,1}**, measures the vertical gap between the convex function and a linear function.
https://arxiv.org/abs/1004.5049
</description>
</card>



<card>
<visual>
RCard-BifunctionalGenEntropy.png
</visual>
<description>
Generalizing Shannon entropy with two smooth monotone functions f and g with f o g^{-1} strictly convex.
Get generalized Kullback-Leibler divergence as cross-entropy minus entropy.
When f(x)=x and g(x)=log(x), f o g^{-1} =exp(x), recover Shannon's entropy.
https://arxiv.org/abs/2001.09660
</description>
</card>


<card>
<visual>
RCard-GLSR-VAE.png
</visual>
<description>
GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures
https://twitter.com/gaetan_hadjeres/status/887322249694511106
</description>
</card>


<card>
<visual>
RCard-InvarianceSL2CauchyFdiv.png
</visual>
<description>
*All* f-divergences between Cauchy distributions are symmetric !!!
Proof uses complex parameter θ=l+is instead of  (location,scale): 
Linear fractional transformation of Cauchy RV= LFT of its complex param. 
Invariance of f-divergences by action of SL(2,R)
https://tinyurl.com/fdivCauchy
</description>
</card>

</Portfolio>