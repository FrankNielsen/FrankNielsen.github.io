<head>
<title>Natural gradient</title>
</head>

<h1>Natural gradient</h1>

The natural gradient descent (NGD) is the steepest descent on a statistical manifold (Fisher-Rao manifold or more generally
 any Riemannian manifold).
 NGD is <A HREF="https://franknielsen.github.io/blog/NaturalGradientConnections/NaturalGradientConnections.pdf">connected</A> with ordinary GD, mirror descent (MD) and Riemannian gradient descent (RGD).
 
<ol>
<li> Natural gradient descent as a first-order Riemannian gradient descent (RGD)
<li> Bregman Mirror descent and natural gradient descent
<li> Natural gradient descent as ordinary gradient descent on dually parameterized function in a dually flat space
</ol>


<h2>Some works and notes on natural gradient methods</h2>
 
<ul>


<li><A HREF="https://arxiv.org/abs/2302.09738" target="_blank">Simplifying Momentum-based Riemannian Submanifold Optimization
  </A>,   arXiv: 2302.09738
  
  <li><A HREF="https://www.neurreps.org/" target="_blank">Practical Structured Riemannian Optimization with Momentum by using 
  Generalized Normal Coordinates</A>, NeurReps Workshop of NeurIPS 2022.
  
  
  <li><A HREF="http://proceedings.mlr.press/v139/lin21e/lin21e.pdf" target="_blank">Tractable structured natural-gradient 
  descent using local parameterizations</A>, ICML 2021: 6680-6691<BR>
  <A HREF="https://arxiv.org/abs/2102.07405">arXiv:2102.07405</A>
  
  <li><A HREF="http://proceedings.mlr.press/v70/sun17b/sun17b.pdf" target="_blank">Relative Fisher Information and Natural Gradient for Learning Large Modular Models</A>,
  ICML 2017: 3289-3298<BR>
  <A HREF="https://arxiv.org/abs/1606.06069">arXiv:1606.06069</A> (Relative Natural Gradient for Learning Large Complex Models)

<li><A HREF="https://arxiv.org/abs/2206.08598">On the Influence of Enforcing Model 
Identifiability on Learning dynamics of Gaussian Mixture Models</A>, arXiv:2206.08598
  

<li><A HREF="https://franknielsen.github.io/blog/NaturalGradientConnections/NaturalGradientConnections.pdf">A note on the natural gradient and its connections with the
Riemannian gradient, the mirror descent, and the ordinary gradient</A>

<li><A HREF="https://yorkerlin.github.io/year-archive/" target="_blank">Natural gradient in machine learning</A>, a collaborative blog by Wu Lin (with feedback from Emtiyaz Khan, Mark Schmidt and Frank Nielsen)



</ul>


<HR>
Frank Nielsen, February 2023
