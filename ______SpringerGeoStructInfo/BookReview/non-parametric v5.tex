%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}
\newcommand{\B}{\mbox{B}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^{n}}
\newcommand{\Emu}{\mbox{E}_{\mu}}
\newcommand{\Ep}{\mbox{E}_{p}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\M}{\mathfrak{M}}
\newcommand{\id}{\mbox{id}}

\newcommand{\Mo}{\mathbb M}
\newcommand{\Eo}{\mathbb E}
\newcommand{\Ro}{\mathbb R}
\newcommand{\Io}{\mathbb I}

\newcommand{\Ddiv}{\mathcal{D}}
\newcommand{\Dfrechet}{d}  %{{\mathfrak D}\,}

\newcommand{\beginproof}{\par\strut\vskip 0.0cm\noindent{\bf Proof}\par}
\renewcommand{\endproof}{\par\strut\hfill$\square$\par\vskip 0.2cm}


\newcommand{\upd}{{\rm d}}



\begin{document}

\title*{Rho-Tau Embedding of Statistical Models} 
 
\author{Jan Naudts and Jun Zhang}
  \authorrunning{Naudts Zhang}
\institute{Jan Naudts \at Universiteit Antwerpen, Antwerpen Belgium,
\email{jan.naudts@uantwerpen.be}
\and Jun Zhang \at University of Michigan, Ann Arbor, MI U.S.A.,
\email{junz@umich.edu}
}

\maketitle  

\abstract{ 
Two strictly increasing functions $\rho$ and $\tau$ determine
 the rho-tau embedding of a statistical model.
The Riemannian metric tensor is derived from the rho-tau divergence.
It depends only on the product $\rho'\tau'$ of the derivatives
of $\rho$ and $\tau$. Hence, once the metric tensor is fixed
still some freedom is left to manipulate the geometry.
We call this the {\it gauge freedom}. A sufficient condition for the
existence of a dually flat geometry is established. It is shown
that, if the coordinates of a parametrized model are affine
then the rho-tau metric tensor is Hessian and the dual
coordinates are affine as well. We illustrate our approach using models
belonging to deformed exponential families,
and give simple and precise characterization for the rho-tau metric to become Hessian.
}




\section{Introduction}
\label{sect:intro}

A {\em statistical manifold} \cite{lauritzen1987a,amarinagaoka2000,AyJLS2017}
is an abstract manifold $\Mo$ equipped with a Riemannian metric $g$ and an
Amari-Chentsov tensor $T$. If the manifold is a smooth differentiable manifold
then it can be realized \cite{LeHV2005} as a {\em statistical model}.

Most studies of statistical models are based on the widely used logarithmic embedding of
probability density functions. Here, more generally embeddings are considered.
Recent work \cite{ZN17,NZ17,NZ18} unifies the formalism of rho-tau embeddings \cite{zhang2004a}
with statistical models belonging to deformed exponential families \cite{NJ04}.
The present exposition continues this investigation.  

The notion of a statistical manifold has been generalized in the non-parametric 
setting \cite{pistonesempi1995,pistonerogantin1999}
to include Banach manifolds. The corresponding terminology is used here, although up to now
only a few papers have combined non-parametrized manifolds with deformed exponential families
\cite{pistone2009,NNJ12,VigCav2013,MP17}.


The rho-tau divergence is discussed in the next section. Eguchi \cite{eguchi1983,eguchi1985}
proved under rather general conditions that, given a differentiable manifold,
a divergence function defines a metric tensor and a pair of connections.
These are derived in Section \ref{sect:tangent}, respectively Section \ref{sect:geometry}.
Parametrized statistical models are discussed in Section \ref{sect:param}, which
discusses Hessian geometry, and Section \ref{sect:deform}, which deals with deformed
exponential families.


\section{Rho-tau divergence}
\label{sect:rhotau}


The points of the statistical manifold $\Mo$ are assumed to be random variables over some measure space ${\cal X}, \mu$.
The expectation of the random variable $X$ is denoted $\Eo_\mu X$.
Given a strictly convex differentiable function $I$ and a pair of real-valued
random variables $P$ and $Q$ the Bregman divergence \cite{bregman1967}
is given by
\beq
\Ddiv (P,Q)&=&\Eo_\mu\left[ I(P)-I(Q)-(P-Q)I'(Q)\right],
\label{div:breg}
\eeq
where $I'$ denotes the derivative of $I$.
A generalization involving two strictly increasing real functions $\rho(u)$ and $\tau(u)$
is proposed in \cite {zhang2004a}. For the sake of completeness the definition is repeated here.
Throughout the text these functions $\rho$ and $\tau$ are assumed to be at least once,
sometimes twice differentiable.


There exists a strictly convex function $f$ with the property that $f'\circ \rho=\tau$.
It is given by
\be
\label{div:f}
f(u)=\int^{\rho^{-1}(u)} \tau(v)\upd \rho(v).
\ee

The convex conjugate function $f^*$ is therefore given by
\be
f^*(u)=\int^{\tau^{-1}(u)}\rho(v)\upd\tau(v),
\label{div:fstar}
\ee
provided the lower boundary of the integrals is chosen appropriately.

The original definition \cite{zhang2004a} of the rho-tau divergence can be written as
\beq
\Ddiv _{\rho,\tau}(P,Q)&=&\Eo_\mu\left[
f(\rho(P))+f^*(\tau(Q))-\rho(P)\tau(Q)\right]
\label{div:rhotaudivzhang}
\eeq
which is assumed to be $\le +\infty.$ The reformulation given below simplifies the proof of some of its properties.

\begin{definition}
Let be given two strictly increasing differentiable functions $\rho$ and $\tau$,
defined on a common open convex domain $D$ in $\Ro$.
%Let $f$ and $f^*$ be defined by (\ref {div:fstar}), respectively (\ref {div:fstar}).
The rho-tau divergence  of two random variables $P$ and $Q$ with values in $D$ is given by
\be
\Ddiv _{\rho,\tau}(P,Q)=\Eo_\mu\left(\int_Q^P\left[\tau(v)-\tau(Q)\right]\upd \rho(v)\right).
\label{div:rhotaudiv}
\ee
\end{definition}

This definition is equivalent to (\ref  {div:rhotaudivzhang}). To see this split (\ref {div:rhotaudiv})
into two parts.
Use (\ref {div:f}) to write the former contribution as $\Eo_\mu f\circ\rho(P)-\Eo_\mu f\circ\rho(Q)$
and the latter as $-\Eo_\mu\tau(Q)[\rho(P)-\rho(Q)]$. Use partial integration to prove
that $f\circ\rho+f^*\circ\tau=\rho\tau$.
%
This definition also generalizes (\ref {div:breg}). To see this take $I=f$, $\rho=\id$, and $\tau=I'$.

Note that the integral in (\ref {div:rhotaudiv}) is a Stieltjes integral, which is well-defined because $\rho$ and $\tau$
are strictly increasing functions. The result is non-negative. Hence, the $\mu$-expectation
is either convergent or it diverges to $+\infty$.

Let $p(\zeta,\eta)$ be the joint probability distribution 
that $P$ has value $\zeta$ and $Q$ has value $\eta$.
Then (\ref {div:rhotaudiv}) can be written as
\beq
\Ddiv _{\rho,\tau}(P,Q)&=&\int p(\zeta,\eta)\upd\zeta\upd\eta\,
\left(\int_\eta^\zeta\left[\tau(v)-\tau(\eta)\right]\upd \rho(v)\right)\cr
&\le&
\int p(\zeta,\eta)\upd\zeta\upd\eta\,|\tau(\zeta)-\tau(\eta)|\,|\rho(\zeta)-\rho(\eta)|\cr
&\le&\left\{\Eo_\mu |\tau(P)-\tau(Q)|^2\Eo_\mu|\rho(P)-\rho(Q)|^2\right\}^{1/2}.
\eeq

\begin{theorem}
$\Ddiv _{\rho,\tau}(P,Q)\ge 0$ with equality if $P=Q$.
If $\mu$ is faithful, i.e. $\Eo_\mu P=0$ implies $P=0$ for any $P$, then
$\Ddiv _{\rho,\tau}(P,Q)=0$ implies $P=Q$.
\end{theorem}

\beginproof

From (\ref {div:rhotaudiv}) it is immediately clear that $\Ddiv _{\rho,\tau}(P,Q)\ge 0$
and $\Ddiv _{\rho,\tau}(P,P)=0$.
Assume now that $\Ddiv _{\rho,\tau}(P,Q)=0$. 
By assumption this implies that
\be
\int_Q^P\left[\tau(v)-\tau(Q)\right]\upd \rho(v)=0\quad\mu\mbox{-almost everywhere}.
\nonumber
\ee
However, because $\tau$ and $\rho$ are strictly increasing the integral is strictly positive unless
$P=Q$, $\mu$-almost everywhere.
\endproof

It can be easily verified that the rho-tau divergence satisfies the following generalized Pythagorian equality for any three points $P,Q,R$
$$
\Ddiv _{\rho,\tau}(P,Q) + \Ddiv _{\rho,\tau}(Q,R) - \Ddiv _{\rho,\tau}(P,R) = \Eo_\mu \left\{ [\rho(P) - \rho(Q)] [ \tau(R) - \tau(Q)]    \right\} .
$$

The general expression for the rho-tau entropy is
\be
S(P)=-\Eo_\mu f(\rho(P))+\mbox{ constant}
=-\Eo_\mu\int^P\tau(u)\upd\rho(u).
\ee
Hence, the following identity holds
\be
\Ddiv _{\rho,\tau}(P,Q)=
-S(P)+S(Q)-\Eo_\mu\left[\rho(P)-\rho(Q)\right]\tau(Q).
\label{div:ent}
\ee
In \cite{ZN17, NZ18}, we also discuss rho-tau cross-entropy, as well as the notion of ``dual entropy'' arising out of rho-tau embedding. 

Rho-tau divergence $\Ddiv _{\rho,\tau}(P,Q)$ is a special form of the more general divergence function
$\Ddiv^{(\alpha)}_{f, \rho}(P,Q)$ arising out of convex analysis, see \cite{zhang2004a, zhang2005}:
\beq
\label{alpha_div}
& &\Ddiv^{(\alpha)}_{f, \rho}(P,Q) 
= \frac{4}{1-\alpha^{2}} \cr
&\times&
\Eo_\mu \left\{
 \frac{1-\alpha}{2} f(\rho(P)) + \frac{1+\alpha}{2}
f(\rho(Q)) -  f \left( \frac{1-\alpha}{2}
\rho(P)+\frac{1+\alpha}{2} \rho(Q) \right) \right\} .\cr
& &
\eeq
Clearly
\begin{eqnarray*}
 \lim_{\alpha \rightarrow 1}  \Ddiv^{(\alpha)}_{f, \rho}(P,Q) &=& 
 \Ddiv_{\rho,\tau}(P,Q) = \Ddiv_{\tau,\rho}(Q,P); \\
 \lim_{\alpha \rightarrow -1}  \Ddiv^{(\alpha)}_{f, \rho}(P,Q) &=& 
 \Ddiv_{\rho,\tau}(Q,P) = \Ddiv_{\tau,\rho}(P,Q); 
\end{eqnarray*}
with $f^\prime \circ \rho = \tau$ 
(and equivalent $(f^\ast)^\prime \circ \tau = \rho$, with $f^\ast$ denoting convex conjugate of $f$).  
Though in $\Ddiv^{(\alpha)}_{f, \rho}(P,Q)$ the two free functions are $f$ 
(a strictly convex function) and $\rho$ (a strictly monotone increasing function), as reflected in its subscripts,
there is only notational difference from the $\rho, \tau$ specification of two function's choice. 
This is because for $f, f^\ast, \rho, \tau$, a choice of any two functions 
(one of which would have to be either $\rho$ or $\tau$) would specify the remaining two. See \cite{zhang2004a,zhang15}. 


\section{Tangent vectors}
\label{sect:tangent}

The rho-tau divergence introduced above can be used to fix a Riemannian metric on the tangent planes
of the statistical manifold $\Mo$.
%
The Fr\'echet derivative of a random variable is again a random variable.
Therefore one can expect that
the tangent vectors at the point $P$ of $\Mo$ are random variables with vanishing expectation value.
The metric tensor is then used to turn the tangent plane $T_P\Mo$ into a (pre-) Hilbert space.

In the standard situation of the Fisher-Rao metric the point $P$ is a probability density function $p^\theta$,
parametrized with $\theta\in\Ro^n$. A short calculation gives
\be
\partial_j\Eo_\mu p^\theta Y=\big\langle\partial_j \log p^\theta,Y\big\rangle_\theta,
\label{tangent:fr}
\ee
with $\langle X,Y\rangle_\theta=\Eo_\mu p^\theta XY$,
and where  $\partial_j$ is an abbreviation for $\partial/\partial\theta^j$.
The metric tensor is then given by 
\be
g_{i,j}(\theta)
=
\langle \partial_i \log p^\theta,\partial_j \log p^\theta\rangle_\theta.
\nonumber
\ee
The score variables $\partial_j \log p^\theta$ have vanishing expectation and span the tangent plane
at the point $p^\theta$.

These expressions are now generalized using the Fr\'echet derivative in non-parametric setting. Let $X$ be random variable. First, its Fr\'echet derivative (directional derivative) $\Dfrechet_X$ is defined as (for a smooth function $h$)
$$
\Dfrechet_X  h(P) := \lim_{\epsilon \rightarrow 0}  \frac{h(P+ \epsilon X)-h(P)}{\epsilon} ,$$
which is a random function linearly dependent on $X$. Now we take one of the two increasing functions $\rho$ and $\tau$, say $\rho$ to define a two-point
correlation function $\Eo_\mu\rho(P)Y$, and the other function, $\tau$, to deform the logarithmic function which appears in the definition of the standard scores.
The expression analogue to (\ref{tangent:fr}) now
involves Fr\'echet derivatives of $\Eo_\mu\rho(P) Y$ and of $\tau(P)$. It becomes
\be
\Dfrechet_X\Eo_\mu\rho(P) Y=\big\langle \Dfrechet_X\tau(P),Y\big\rangle_P,
\label{tangent:frgen}
\ee
with
\be
\left\langle X,Y\right\rangle_P=\Eo_\mu\frac{\rho'(P)}{\tau'(P)}XY.
\nonumber
\ee
This relation should hold for any $P$ in $\Mo$ and $X$ in $T_P\Mo$, and for any random variable $Y$. 
The metric tensor becomes
\beq
g_{X,Y}(P)
&=&\big\langle \Dfrechet_X\tau(P),\Dfrechet_Y\tau(P)\big\rangle_P\cr
&=&\Eo_\mu\rho'(P)\tau'(P)\Dfrechet_X P\Dfrechet_Y P.
\label{tangent:gexpl}
\eeq
This metric tensor is related to the divergence function introduced in the previous
section by
\be
\Dfrechet^{\mbox{\tiny P}}_Y\Dfrechet^{\mbox{\tiny Q}}_X \Ddiv _{\rho,\tau}(P,Q)
\bigg|_{P=Q}
=-g_{X,Y}(P),
\nonumber
\ee
where $\Dfrechet^{\mbox{\tiny P}}$ is the Fr\'echet derivative acting only on $P$ and 
$\Dfrechet^{\mbox{\tiny Q}}$ acts only on $Q$. See \cite{zhang13} for derivation of the metric tensor in the form of \label{tangent:gexpl} for the non-parametric setting. 

\section{Gauge freedom}
\label{sect:gauge}

From (\ref {tangent:gexpl}) it is clear that the metric tensor depends only
on the product $\rho'\tau'$ and not on $\rho$ and $\tau$ separately.
This  implies that once the metric tensor is fixed there remains one 
function to be chosen freely, either the embedding $\rho$ or the 
deformed logarithm $\tau$, keeping $\rho'\tau'$ fixed.
This is what we call the gauge freedom of the rho-tau formalism.

It is known for long that distinct choices of the divergence function
can lead to the same metric tensor. The present formalism offers
the opportunity to profit from this freedom. Quantities such as
the divergence function, the entropy or the alpha-family
of connections depend on the specific choice of both $\rho$ and $\tau$.
This is illustrated further on.


The {\em rho-affine gauge} is characterized by $\rho=\id$ .
Several of the generalized Bregman divergences found in the literature
belong to this case. The phi-divergence of \cite{NJ04} is obtained
by choosing $\tau$ equal to the deformed logarithm $\log_\phi$ (see Section \ref {sect:deform}),
the derivative of which is $1/\phi$. This implies $\rho'\tau'=1/\phi$,
which is also the condition for the deformed metric tensor of \cite{NJ04}
to be conformally equivalent with (\ref {tangent:gexpl}).
The U-divergence of \cite{ES06} is obtained by taking $\tau$ equal to the
inverse function of $U'$. These were discussed in \cite{ZN17,NZ17,NZ18}.

Also of interest is the gauge defined by $\rho(u)=1/\tau'(u)$.
Let $\log_\rho$ be the corresponding deformed logarithm (see (\ref {deform:log}) below).
It satisfies $\log_\rho(u)=\tau(u)-\tau(1)$. Hence, the entropy becomes
\be
S(P)=-\Eo_\mu\rho(P)\tau(P)+\Eo_\mu P+\mbox{ constant}.
\nonumber\ee
The divergence becomes
\beq
\Ddiv _{\rho,\tau}(P,Q)
% &=&
% \Eo_\mu\rho(P)\tau(P)-\Eo_\mu P
% -\Eo_\mu\rho(Q)\tau(Q)+\Eo_\mu Q\cr
% & &
% -\Eo_\mu\left[\rho(P)-\rho(Q)\right]\tau(Q)\cr
&=&
\Eo_\mu\rho(P)\left[\log_\rho(P)-\log_\rho(Q)\right]-\Eo_\mu\left[P-Q\right].
\nonumber
\eeq
This expression is an obvious generalization of the Kullback-Leibler divergence.


\section{Induced geometry}
\label{sect:geometry}

A divergence function not only fixes a metric tensor by taking two derivatives,
it also fixes a pair of torsion-free connection by taking an extra
derivative w.r.t.~the first argument
\cite{eguchi1983} \cite{eguchi1985}. In particular,
the rho-tau-divergence  (\ref {div:rhotaudiv}) determines an alpha-family of
connections \cite{zhang2004a,zhang13,NZ17}.

A covariant derivative is defined by
\be
\langle \nabla_Z\Dfrechet_X\tau(P),\Dfrechet_Y\tau(P)\rangle_P
=
-\Dfrechet^{\mbox{\tiny P}}_Z
\Dfrechet^{\mbox{\tiny P}}_Y\Dfrechet^{\mbox{\tiny Q}}_X \Ddiv _{\rho,\tau}(P,Q)
\bigg|_{Q=P}.
\nonumber
\ee
A short calculation gives
\beq
\langle\nabla_Z\Dfrechet_X\tau(P),\Dfrechet_Y\tau(P)\rangle_P
&=&
\Eo_\mu \left[\Dfrechet_X\tau(P)\right]
\,\Dfrechet_Z\Dfrechet_Y\rho(P).
\nonumber
\eeq
Let $\nabla^{(1)}_Z=\nabla_Z$ and let $\nabla_Z^{(-1)}$ be the operator
obtained by interchanging $\rho$ and $\tau$.
This is
\beq
\langle\nabla_Z^{(-1)}\Dfrechet_X\tau(P),\Dfrechet_Y\tau(P)\rangle_P
&=&
\Eo_\mu \left[\Dfrechet_X\rho(P)\right]
\,\Dfrechet_Z\Dfrechet_Y\tau(P)\cr
&=&
\langle \Dfrechet_X\tau(P)
,\Dfrechet_Z\Dfrechet_Y\tau(P)\rangle_P.
\label{ind:grad-1}
\eeq
This shows that $\nabla_Z^{(-1)}$ is the hermitian conjugate of $\Dfrechet_Z$.
In addition one has
\be
\langle\nabla_Z^{(1)}\Dfrechet_X\tau(P),\Dfrechet_Y\tau(P)\rangle_P
+\langle\Dfrechet_X\tau(P),\nabla_Z^{(-1)}\Dfrechet_Y\tau(P)\rangle_P
=\Dfrechet_Z\, g_{XY}(P).
\label{ind:dual}
\ee
The latter expression shows that the connections $\nabla^{(1)}$
and $\nabla^{(-1)}$ are the dual of each other with respect to $g$.
%
The alpha-family of connections is then obtained by linear interpolation with $\alpha \in [-1,1]$
\be \label{alpha_connect}
\nabla^{(\alpha)}_Z=\frac{1+\alpha}{2}\nabla^{(1)}_Z+\frac{1-\alpha}{2}\nabla^{(-1)}_Z ,
\ee
such that the covariant derivatives $\nabla^{(\alpha)}$ and $\nabla^{(-\alpha)}$ are mutually dual. 
In particular, $\nabla^{(0)}$ is self-dual and therefore coincides with the Levi-Civita
connection. The family of $\alpha$-connections (\ref{alpha_connect}) is induced by the divergence function
$\Ddiv^{(\alpha)}_{f, \rho}(P,Q)$ given by (\ref{alpha_div}), with corresponding $\alpha$-values. Furthermore, upon switching $\rho \leftrightarrow \tau$ in the divergence function, the designation of 1-connection vs (-1)-connection also switches. 

From (\ref {ind:grad-1}) it is clear that the covariant derivative $\nabla_Z^{(-1)}$
vanishes on the tangent plane when
\be
\langle \Dfrechet_X\tau(P)
,\Dfrechet_Z\Dfrechet_Y\tau(P)\rangle_P=0
\quad\mbox{ for all }X,Y\in T_P\Mo.
\label{ind:flat}
\ee
If this holds for all $P$ in $\Mo$ then the $\nabla_Z^{(-1)}$-geometry is flat.
This implies that the dual geometry $\nabla_Z^{(1)}$ is also flat --- see Theorem 3.3 of \cite{amarinagaoka2000}.
The interpretation of (\ref {ind:flat}) is that all second derivatives
$\Dfrechet_Z\Dfrechet_Y\tau(P)$ are orthogonal to the tangent plane.

\section{Parametrized Models}
\label{sect:param}

The previous sections deal with the geometry of arbitrary manifolds consisting of random variables,
without caring whether they possess special properties. Now parametrized models 
with a Hessian  metric $g$ are considered.

From here on the random variables of the manifold $\Mo$ are probability distribution functions $p^\theta$,
labeled with coordinates $\theta$ belonging to some open convex subset $U$ of $\Ro^n$.
The manifold is assumed to be differentiable. In particular, the $\theta^i$ are covariant coordinates.
The simplifications induced by this setting are that the tangent planes are finite-dimensional
and that dual coordinates belong again to $\Ro^n$. For general Banach manifolds
both properties need not to hold.

The tangent plane $T_\theta\Mo\equiv T_{p^\theta}\Mo$
now has a finite basis of vectors $\partial_i\tau(p^\theta) = \tau^\prime(p^\theta) \partial_i$, which is isomorphic to (in fact iso-direction with) $\partial_i$, and hence linearly independent. The metric tensor 
\be
g_{i,j}(\theta)=\langle \partial_i\tau(p^\theta),\partial_j\tau(p^\theta)\rangle_\theta
\nonumber
\ee
is a strictly positive-definite matrix.

The metric $g$ of the manifold $\Mo$ is said to be Hessian if there exists a strictly convex function
$\Phi(\theta)$ with the property that
$g_{i,j}(\theta)=\partial_i\partial_j\Phi(\theta)$.
Let $\Psi(\eta)$ denote the convex dual of $\Phi(\eta)$. This is
\be
\Psi(\eta)=\sup_{\theta}\{\langle\eta,\theta\rangle-\Phi(\theta):\,\theta\in U\}.
\nonumber
\ee
Let $U^*$ denote the subset of $\Ro^n$ of $\eta$ for which the maximum is reached
at some $\theta$ in $U$. This $\theta$ is unique and defines a bijection
$\theta\mapsto\eta$ between $U$ and $U^*$. These $\eta$ are dual coordinates for the manifold $\Mo$.
Conversely \cite{NZ17}, if there exist
coordinates $\eta_i$ for which $g_{i,j}(\theta)=\partial_j\eta_i$ then 
the rho-tau metric tensor $g$ is Hessian.


The condition (\ref {ind:flat}) for $\nabla^{(-1)}$ to vanish can now be written as
\be
\langle\partial_i\tau(p^\theta),\partial_k\partial_j\tau(p^\theta)\rangle_\theta=0,
\quad\mbox{ for all }\theta\in U\mbox{ and for all }i,j,k.
\label{ind:flat2}
\ee

\begin{theorem}
Assume that the $\theta^i$ are affine coordinates such that $\Gamma^{(-1)} (\theta) = 0$. 
Then 
\begin{description}
 \item 1) the metric tensor $g$ is Hessian;
 \item 2) the $\eta_i$ are affine coordinates for the connection $\Gamma^{(1)}$.
\end{description}
 
\end{theorem}

\beginproof

%\paragraph{1)}
1)
Use (\ref {ind:flat2}) and the duality relation (\ref{ind:dual}) to obtain
\be
\langle\partial_k\partial_i\tau(p^\theta),\partial_j\tau(p^\theta)\rangle_\theta
=\partial_k g_{i,j}(\theta).
\nonumber
\ee
This implies that $\partial_k g_{i,j}(\theta)=\partial_i g_{k,j}(\theta)$.
Hence there exist functions $\eta_j(\theta)$ such that
$g_{i,j}(\theta)=\partial_i\eta_j(\theta)$. As remarked above, it is proved in
\cite{NZ17} that this suffices to conclude that the metric $g$ is Hessian.


%\paragraph{2)}
2)
Let us show that
\be
\eta(t)=(1-t)\eta^{(1)}+t\eta^{(2)}.
\label{geom:etaflat}
\ee
is a solution of the Euler-Lagrange equations
\be
\frac{\upd^2\,}{\upd t^2}\theta^i
+\Gamma^i_{k,m}
\left(\frac{\upd\,}{\upd t}\theta^k\right)
\left(\frac{\upd\,}{\upd t}\theta^m\right)
=0.
\label{geom:EulLagr}
\ee
Here, the $\Gamma^i_{k,m}$ are the coefficients of the connection $\Gamma^{(1)}$.
They follow from
\be
\Gamma_{ij,k}=\partial_ig_{j,k}(\theta).
\label {geom:omega1}
\ee
One has
\be
\frac{\upd\,}{\upd t}\theta^i
=\frac{\partial\theta^i}{\partial\eta_j}\frac{\upd\eta_j}{\upd t}
=g^{ij}(\theta)\left[\eta^{(2)}_j-\eta^{(1)}_j\right]
\nonumber
\ee
and
\beq
\frac{\upd^2\,}{\upd t^2}\theta^i
&=&
\frac{\upd\,}{\upd t}g^{ij}(\theta)\left[\eta^{(2)}_j-\eta^{(1)}_j\right]\cr
&=&
\left[\partial_kg^{ij}(\theta)\right]\frac{\upd\theta^k}{\upd t}\left[\eta^{(2)}_j-\eta^{(1)}_j\right]\cr
&=&
\left[\partial_kg^{ij}(\theta)\right]g^{kl}(\theta)
\left[\eta^{(2)}_l-\eta^{(1)}_l\right]
\left[\eta^{(2)}_j-\eta^{(1)}_j\right]\cr
&=&
\left[\partial_kg^{ij}(\theta)\right]g_{jm}(\theta)
\left(\frac{\upd\,}{\upd t}\theta^k\right)
\left(\frac{\upd\,}{\upd t}\theta^m\right).
\nonumber
\eeq
The l.h.s.~of (\ref {geom:EulLagr}) becomes
\be
\mbox{l.h.s.}
=
\left\{
\left[\partial_kg^{ij}(\theta)\right]g_{jm}(\theta)
+\Gamma^i_{km}
\right\}
\left(\frac{\upd\,}{\upd t}\theta^k\right)
\left(\frac{\upd\,}{\upd t}\theta^m\right).
\nonumber
\ee
This vanishes because (\ref {geom:omega1}) implies
\be
\Gamma^i_{km}=-\left[\partial_kg^{ij}(\theta)\right]g_{jm}(\theta).
\nonumber
\ee

\endproof


\section{The deformed exponential family}
\label{sect:deform}

A repeated measurement of $n$ independent random variables $F_1,\cdots,F_n$ results
in a joint probability distribution $\pi(\zeta_1,\cdots,\zeta_n)$, which describes
the probability that the true value of the measured data equals $\zeta$.
A model belonging to the exponential family can be used to approximate
the empirical data. More generally, the model can be taken in a deformed
exponential family, obtained by using a deformed exponential function $\exp_\phi$.
Following \cite{NJ04}, a deformed logarithm $\log_\phi$ is defined by
\be
\log_\phi(u)=\int_1^u\upd v\,\frac{1}{\phi(v)},
\label{deform:log}
\ee
where $\phi(v)$ is strictly positive and integrable on the open interval $(0,+\infty)$.
The deformed exponential function $\exp_\phi(u)$ is the inverse function of $\log_\phi(u)$.
It is defined on the range $\cal R$ of $\log_\phi(u)$, but is 
eventually extended with the value 0 if $u<{\cal R}$ and with the value $+\infty$
if $u>{\cal R}$. 

The expression for the probability density function then becomes
\be \label{deform_phi}
p^\theta(x)=\exp_\phi\left(\sum_{k=1}^n\theta^kF_k(x)-\alpha(\theta)\right).
\ee
The function $\alpha(\theta)$ serves to normalize $p^\theta$ and is assumed to exists
within the open convex domain $U\subset\Ro^n$ in which the model is defined.
One can show \cite{NJ04} that it is a convex function.
However, in general it does not coincide with the potential $\Phi(\theta)$ of the previous section.
The explanation is that {\em escort probabilities} come into play.
Indeed, from 
\be
0=\partial_i\Eo_\mu p^\theta=\Eo_\mu\phi(p^\theta)\left[F_i-\partial_i\alpha\right]
\nonumber
\ee
follows that
\be
\partial_i\alpha=\tilde \Eo_\theta F_i,
\nonumber
\ee
with the escort expectation $\tilde\Eo_\theta$ defined by
\be
\tilde\Eo_\theta Y=\frac{\Eo_\mu\phi(p^\theta)Y}{\Eo_\mu\phi(p^\theta)}.
\nonumber
\ee
Only in the non-deformed case, when $\phi(u)=u$, the escort  $\tilde\Eo_\theta$
coincides with the model expectation $\Eo_\theta$. Then the dual coordinates
$\eta_i$ satisfy $\eta_i=\Eo_\theta F_i=\partial_i\alpha(\theta)$.

In general, the rho-tau metric tensor $g$ of the deformed exponential model is {\em not}
Hessian.We have the following Theorem (see \cite{NZ18}) 

\begin{theorem}
With respect to the (deformed) $\phi$-exponential family $p^\theta$ obeying  (\ref{deform_phi}), the rho-tau metric tensor $g$ is Hessian if and only if 
$$
\rho'\tau'\phi=\id .
$$ 
\end{theorem}
  
In such a situation,  the rho-tau metric tensor is conformally equivalent
with the metric tensor obtained by taking the Hessian of the normalization
function $\alpha$; for the latter the potential $\Phi(\theta)$ is constructed in \cite{NJ04}. However, there still leaves a gauge freedom. The question is then whether one can choose $\rho$ and $\tau$ so that condition 
(\ref {ind:flat}) for the dually flat geometry is satisfied.
A sufficient condition is that $\rho=\id$ and $\tau=\log_\phi$. This is the rho-affine gauge. In this gauge both the $\theta^i$ and the $\eta_i$ coordinates are affine and the model has a dually flat structure.


% \section{Discussion}
% \label{sect:discuss}

\vspace{0.1cm}

\noindent {\bf Acknowledgement} \\
The research reported here is supported by DARPA/ARO Grant W911NF-16-1-0383 (PI: Jun Zhang). 

\section*{}
\begin{thebibliography}{99}


\bibitem{amarinagaoka2000}
Amari, S., Nagaoka, H.: Methods of Information Geometry. AMS Monograph, 
Oxford University Press, 2000. (Originally published in Japanese by Iwanami 
Shoten, Tokyo, Japan, 1993.) 

\bibitem{AyJLS2017}
Ay N., Jost, J., L\^E, H.V., Schwachh\"ofer, L.:
Information Geometry. (Springer, 2017)

\bibitem{bregman1967}
Bregman, L.M.: The relaxation method of finding the common point of convex sets 
and its application to the solution of problems in convex programming. {\em USSR 
Comput. Math. Phys.} {\bf 7}0, 200--217 (1967).

\bibitem{eguchi1983}
Eguchi, S.: Second order efficiency of minimum contrast estimators in a curved 
exponential family. {\em Ann. Stat.} {\bf 11}, 793--803 (1983).

\bibitem{eguchi1985}
Eguchi, S.: A differential geometric approach to statistical inference on the 
basis of contrast functionals. {\em Hiroshima Math. J.}  {\bf 15}, 
341--391 (1985).

\bibitem {ES06}
Eguchi, S.:
Information geometry and statistical pattern recognition.
{\em Sugaku Expositions} (Amer. Math. Soc.) {\bf 19},  197--216 (2006) 
(originally S{\rm \=u}gaku 56 (2004) 380  in Japanese).

\bibitem{lauritzen1987a}
Lauritzen, S.: Statistical manifolds. In {\em Differential Geometry in 
Statistical Inference}; Amari, S., Barndorff-Nielsen, O., Kass, R., Lauritzen, 
S., Rao, C.R., Eds.; IMS: Hayward, CA, USA,  Lecture Notes {\bf 10},  
163--216 (1987).

\bibitem{LeHV2005}
L\^e, H.V.:
Statistical manifolds are statistical models.
{\em J. Geom.} {\bf  84}  83 -- 93 (2005).


\bibitem{MP17} Montrucchio, L., Pistone, G.:
Deformed exponential bundle: The linear growth case.
In: {\em Geometric Science of Information,} 
GSI 2017 LNCS proceedings,
F. Nielsen and F. Barbaresco eds., (Springer, 2017), p. 239--246.

\bibitem{NJ04}
Naudts, J.:
Estimators, escort probabilities, and phi-exponential families in statistical physics.
{\em J. Ineq. Pure Appl. Math.} {\bf 5}, 102 (2004).

\bibitem{NZ17}
Naudts, J., Zhang, J.: Information geometry under monotone embedding. Part II: Geometry.
in: {\em Geometric Science of Information,}
GSI 2017 LNCS proceedings,
F. Nielsen and F. Barbaresco eds., (Springer, 2017), p.215--222.

\bibitem{NZ18}
Naudts, J., Zhang J.:Information Geometry Under Monotone Embedding.
Preprint 2018.

\bibitem {NNJ12}
Newton, N. J.:
An infinite-dimensional statistical manifold modeled on Hilbert space.
{\em J. Funct. Anal.} {\bf 263}, 1661--1681 (2012).

\bibitem{pistonesempi1995}
Pistone, G., Sempi, C.: An infinite dimensional geometric structure on the space 
of all the probability measures equivalent to a given one. {\em Ann. Stat.} {\bf 33}, 1543--1561 (1995).

\bibitem{pistonerogantin1999}
Pistone G., Rogantin M.P.: The exponential statistical manifold: Mean parameters, 
orthogonality and space transformations. {\em Bernoulli} {\bf 5}, 
721--760 (1999).

\bibitem{pistone2009}
Pistone, G.: $\kappa$-exponential models from the geometrical viewpoint. 
{\em Eur. Phys. J. B} {\bf 70}, 29--37 (2009).

\bibitem {VigCav2013}
Vigelis, R.F., Cavalcante, C.C.:
On $\phi$-Families of Probability Distributions.
{\em J. Theor. Probab.} {\bf 26}, 870--884 (2013).

\bibitem{zhang2004a}
Zhang, J.: Divergence function, duality, and convex analysis. {\em Neural 
Comput.} {\bf 16}, 159--195 (2004).

\bibitem{zhang2005}
Zhang, J.:
Referential duality and representational duality on statistical manifolds.
In {\it Proceedings of the Second International Symposium on Information Geometry and Its Applications, Tokyo},
Japan, 2005, pp. 58--67.

\bibitem{zhang13}
Zhang, J.: Nonparametric Information Geometry: From Divergence
Function to Referential-Representational Biduality on
Statistical Manifolds.
{\em Entropy} {\bf 15} 1 (2013).

\bibitem{zhang15}
Zhang, J.: On monotone embedding in information geometry.
{\em Entropy}  {\bf 17}, 4485--4499 (2015).

\bibitem{ZN17}
Zhang, J., Naudts, J.: Information geometry under monotone embedding. Part I: Divergence functions.
in: {\em Geometric Science of Information,}
GSI 2017 LNCS proceedings,
F. Nielsen and F. Barbaresco eds., (Springer, 2017), p.205--214. 


\end{thebibliography}


\end{document}
