<h1>Distances</h1>



<!--
start from 2017
https://dblp.uni-trier.de/pid/n/FrankNielsen.html
-->

My work on distances has been organized below as follows: 
<ul>
<li>novel definitions of distances, 
<li>generalizations or extensions of distances, and 
<li>calculations or approximations of distances.
</ul>

See also the 
<A HREF="FrankNielsen-distances-figs.pdf" target="_blank">
poster
</A> 
showing historically the statistical distances, and a 
<A HREF="Fig-divhierarchyformula.pdf" target="_blank">
modern overview of distances
</A>.


<ul>

<li><font size="+2">Novel distances</font>:

<ul>



<li>
<A HREF="https://www.mdpi.com/1099-4300/22/9/1019" target="_blank"><b>
The Siegel–Klein Disk: Hilbert Geometry of the Siegel Disk Domain
</A>
<p style="background-color:powderblue;">
Propose the <em>Siegel-Klein distance</em> which is defined as the Hilbert metric distance induced by
the open bounded convex Siegel disk.
Report closed-form expressions in particular cases, and fast guaranteed approximations.
</p>


<li>
<A HREF="https://www.mdpi.com/1099-4300/22/7/713" target="_blank"><b>
On Voronoi Diagrams on the Information-Geometric Cauchy Manifolds
</A>

<p style="background-color:powderblue;">
Propose the <em>Bregman-Cauchy divergence</em> which is a <b>Bregman-Tsallis divergence</b> (for q=2).
</p>

<li>
<A HREF="https://www.mdpi.com/1099-4300/21/5/485" target="_blank"><b>
On the Jensen-Shannon Symmetrization of Distances Relying on Abstract Means</A>

<p style="background-color:powderblue;">
Given two abstract means M and N, we define the <em>(M,N)-Jensen-Shannon divergence</em>.
Show how to choose the abstract mean M in order to get closed-formulas when distributions 
belong to exponential families (<em>Geometric Jensen-Shannon divergence</em>), 
t-Student families (<em>Power Jensen-Shannon divergences</em>), etc.
</p>



<li>
<A HREF="https://www.researchgate.net/publication/335061874_The_Bregman_chord_divergence" target="_blank"><b>
The Bregman Chord Divergence</A>

<p style="background-color:powderblue;">
<em>Bregman chord divergence</em>
</p>



<li>
<A HREF="https://www.researchgate.net/publication/335159741_Geometry_and_Fixed-Rate_Quantization_in_Riemannian_Metric_Spaces_Induced_by_Separable_Bregman_Divergences" target="_blank"><b>
Geometry and Fixed-Rate Quantization in Riemannian Metric Spaces Induced by Separable Bregman Divergences 
</A>

<p style="background-color:powderblue;">
Consider Bregman divergences for separable generators and study the Riemannian distance induced by the Hessian of the Bregman generator:
The <em>Bregman-Riemannian distance</em> which amounts to the Euclidean distance after reparameterization. (The Hessian of F is the Euclidean metric).
 
</p>

<li>
<A HREF="https://www.researchgate.net/publication/335231528_The_Statistical_Minkowski_Distances_Closed-Form_Formula_for_Gaussian_Mixture_Models" target="_blank"><b>
The Statistical Minkowski Distances: Closed-Form Formula for Gaussian Mixture Models
</A>

<p style="background-color:powderblue;">
The <em>statistical Minkowski distances</em> are derived from the Minkowski inequalities.
</p>


<li>
<A HREF="https://arxiv.org/abs/1909.08857" target="_blank"><b>
A note on the quasiconvex Jensen divergences and the quasiconvex Bregman divergences derived thereof<b></a>. arxiv  

<p style="background-color:powderblue;">
Define the <em>quasiconvex Jensen divergence</em> and the <em>quasiconvex Bregman divergences</em> in the limit cases.
</p>


<li>
<A HREF="https://arxiv.org/abs/1812.08113" target="_blank"><b>
On The Chain Rule Optimal Transport Distance
<b></a>. arxiv  

<p style="background-color:powderblue;">
<em>Chain Rule Optimal Transport</em> (CROT). When the base distance is joint convex, use a linear program to define the CROT distance between two mixtures.

</p>



[j47]		Frank Nielsen, Ke Sun, Stéphane Marchand-Maillet:
On Hölder Projective Divergences. Entropy 19(3): 122 (2017)
	[j46]		Frank Nielsen, Richard Nock:
MaxEnt Upper Bounds for the Differential Entropy of Univariate Continuous Distributions. IEEE Signal Process. Lett. 24(4): 402-406 (2017)
	[j45]		Frank Nielsen, Richard Nock:
Generalizing Skew Jensen Divergences and Bregman Divergences With Comparative Convexity. IEEE Signal Process. Lett. 24(8): 1123-1127 (2017)

	Gaëtan Hadjeres, Frank Nielsen:
Deep rank-based transposition-invariant distances on musical sequences. CoRR abs/1709.00740 (2017)



[c114]		Boris Muzellec, Richard Nock, Giorgio Patrini, Frank Nielsen:
Tsallis Regularized Optimal Transport and Ecological Inference. AAAI 2017: 2387-2393
Frank Nielsen, Ke Sun:
Combinatorial bounds on the α-divergence of univariate mixture models. ICASSP 2017: 4476-4480



</ul>


<li><font size="+2">Generalizations of distances</font>:
<ul>


<li><A HREF="https://arxiv.org/abs/2001.09660" target="_blank"><b>
A generalization of the α-divergences based on comparable and distinct weighted means.
</b></a>. arxiv  



<p style="background-color:powderblue;">
α-divergences are scaled difference of an arithmetic weighted mean minus a geometric weighted mean.
Consider comparable weighted means to define <b>generalized α-divergences</b>. 
In the limit of α=1 or α=0, we get <b>generalized Bregman divergences</b>.
</p>

<li><A HREF="https://www.mdpi.com/1099-4300/22/2/221" target="_blank"><b>On a Generalization of the Jensen-Shannon Divergence and the Jensen-Shannon Centroid</b></a>. Entropy 22(2): 221 (2020)

<p style="background-color:powderblue;">
Define the <b>vector-skewed Jensen-Shannon divergences</b> (belonging to the class of f-divergences) 
and propose a convex-concave procedure (CCCP) to calculate the Jensen-Shannon centroids  sets of categorical distributions.
</p>




</ul>

<li><font size="+2">Calculation/approximation of distances</font>:
<ul>

<li><A HREF="https://arxiv.org/abs/2003.02469" target="_blank"><b>
Cumulant-free closed-form formulas for some common (dis)similarities between densities of an exponential family
</b></a>. arxiv  

<p style="background-color:powderblue;">
Jensen divergence and Bregman divergences are defined modulo affine terms of their generators.
Consider the equivalent generator F=-log(p(x;theta)) and  derive simple ways to compute common distances.
Show that the KLD between two densities of an exponential family can be written as the sum of log density ratios.
</p>

<li>
<A HREF="https://arxiv.org/abs/1903.05818" target="_blank"><b>
On power chi expansions of f-divergences</b>
</a>. arxiv  

<p style="background-color:powderblue;">
When the generator of a f-divergence is analytic, use the Taylor expansion of order k and its remainder to approximate f-divergences.
</p>

<li>
<A HREF="https://arxiv.org/abs/1904.10428" target="_blank"><b>
On the Kullback-Leibler divergence between location-scale densities
<b></a>. arxiv  

<p style="background-color:powderblue;">
Reparameterization in the integrals yields distance identities for the Kullback-Leibler divergence.
</p>


<li>
<A HREF="https://arxiv.org/abs/1905.10965" target="_blank"><b>
A closed-form formula for the Kullback-Leibler divergence between Cauchy distributions
<b></a>. arxiv  

<p style="background-color:powderblue;">
Report the closed-form formula for the Kullback-Leibler divergence between two Cauchy distributions:

The KLD between two Cauchy densities is symmetric. 
</p>

<li>
<A HREF="https://arxiv.org/abs/1905.10965" target="_blank"><b>
https://www.researchgate.net/publication/328983644_GUARANTEED_DETERMINISTIC_BOUNDS_ON_THE_TOTAL_VARIATION_DISTANCE_BETWEEN_UNIVARIATE_MIXTURES<b></a>. 

<p style="background-color:powderblue;">
 
</p>




</ul>


 



</ul>
