% Frank.Nielsen@acm.org

% Can we choose alpha so that the FIM is easier to calculate (eg. Cauchy).
% meaning derivation are easier

\documentclass[11pt]{article}
\usepackage{fullpage,amssymb,amsmath,hyperref,url}

\def\eqdef{:=}
\def\eqnota{:=:}
\def\dnu{\mathrm{d}\nu}
\def\calX{\mathcal{X}}
\def\bbR{\mathbb{R}}
\def\Var{\mathrm{Var}}
\def\leftsup#1{{}^{#1}}


\title{The $\alpha$-representations of the Fisher Information Matrix\\ --- On equivalent expressions of the FIM ---}

\date{19 September 2017\\ Revised September 2020}

\author{Frank Nielsen\\ {\tt Frank.Nielsen@acm.org}}

\begin{document}
\maketitle


%This column is also available in pdf: filename \url{alpha-FIM.pdf} 
%\vskip 0.5cm

The {\em Fisher Information Matrix}~\cite{IG-2014} (FIM) for a family of {\em parametric} probability models $\{p(x;\theta)\}_{\theta\in\Theta}$ 
(densities $p(x;\theta)$ expressed with respect to a positive base measure $\nu$) indexed by
 a $D$-dimensional parameter vector $\theta\eqdef(\theta^1,\ldots,\theta^D)$  is historically defined by

\begin{equation}\label{eq:FIM1}
I(\theta) \eqdef [I_{ij}(\theta)],
\quad I_{ij}(\theta) \eqdef E_{p(x;\theta)}\left[\partial_i l(x;\theta)\partial_j l(x;\theta)\right],
\end{equation}
where $l(x;\theta)\eqdef\log p(x;\theta)$ is the {\em log-likelihood function}, and 
$\partial_i \eqnota \frac{\partial}{\partial\theta^i}$ (by notational convention).
The FIM is a $D\times D$ positive semi-definite matrix for a $D$-order parametric family.

The FIM  is a cornerstone in statistics and occurs in many places, like for example the celebrated
 {\em Cram\'er-Rao lower bound}~\cite{CR-2013} for an unbiased estimator $\hat\theta$:
$$
\Var_{p(x;\theta)}[\hat{\theta}]\succeq I^{-1}(\theta),
$$
where $\succeq$ denotes the L\"owner @artial ordering of positive semi-definite matrices: 
$A\succeq B$  iff. $A-B\succ 0$ is  positive semi-definite.
Another use of the FIM is in gradient descent method using the {\em natural gradient} (see~\cite{RFIM-2017}  for its use in deep learning).

Yet, it is common to encounter another equivalent expression of the FIM in the literature~\cite{CR-2013,IG-2014}:

\begin{equation}\label{eq:FIM0}
I_{ij}'(\theta) \eqdef 4\int \partial_i\sqrt{p(x;\theta)}\partial_j\sqrt{p(x;\theta)}\dnu(x)
\end{equation}
This form of the FIM is well-suited to prove that the FIM is always positive semi-definite matrix~\cite{IG-2014}: $I(\theta)\succeq 0$.



It turns out that one can define a family of equivalent representations of the FIM using the {\em $\alpha$-embeddings} of the parametric family.
We define the {\em $\alpha$-representation} of densities $l^{(\alpha)}(x;\theta):=k_\alpha(p(x;\theta))$ with


\begin{equation}\label{eq:alphaembedding}
k_\alpha(u) \eqdef \left\{
\begin{array}{ll}
\frac{2}{1-\alpha}u^{\frac{1-\alpha}{2}}, & \mbox{if $\alpha\not=1$}\\
\log u, & \mbox{if $\alpha=1$}.
\end{array}
\right.
\end{equation}

The function $l^{(\alpha)}(x;\theta)$ is called the {\em $\alpha$-likelihood function}.

The $\alpha$-representation of the FIM (or $\alpha$-FIM for short) is
\begin{equation}\label{eq:FIMalpha}
\boxed{I_{ij}^{(\alpha)}(\theta) \eqdef \int \partial_i l^{(\alpha)}(x;\theta)\partial_j l^{(-\alpha)}(x;\theta) \dnu(x)}
\end{equation}
In compact notation, we have $I_{ij}^{(\alpha)}(\theta)=\int  \partial_i l^{(\alpha)}\partial_j l^{(-\alpha)} \dnu(x)$ (this is the $\alpha$-FIM).
We can expand the $\alpha$-FIM expressions as follows
$$
I_{ij}^{(\alpha)}(\theta) = \left\{
\begin{array}{ll}
\frac{1}{1-\alpha^2}\int \partial_i p(x;\theta)^{\frac{1-\alpha}{2}}  \partial_j p(x;\theta)^{\frac{1+\alpha}{2}} \dnu(x) & \mbox{for $\alpha\not =\pm1$}\\
\int \partial_i \log p(x;\theta) \partial_j p(x;\theta) \dnu(x) & \mbox{for $\alpha\in \{-1,1\}$}\\
%\int \partial_i p(x;\theta)\partial_j \log p(x;\theta) \dnu(x) & \mbox{for $\alpha=-1$}
\end{array}
\right.
$$


The proof that $I_{ij}^{(\alpha)}(\theta)=I_{ij}(\theta)$ follows from the fact that
$$
\partial_i l^{\alpha} =  p^{-\frac{\alpha+1}{2}}\partial_i p = p^{\frac{1-\alpha}{2}}\partial_i l,
$$
since $\partial_i l=\frac{\partial_i p}{p}$.

Therefore we get
$$
\partial_i l^{(\alpha)}\partial_j l^{(-\alpha)}=p\partial_i l\partial_j l,
$$
and $I_{ij}^{(\alpha)}(\theta)=E[\partial_i l\partial_j l]=I_{ij}(\theta)$.

Thus Eq.~\ref{eq:FIM1} and Eq.~\ref{eq:FIM0} where two examples of the $\alpha$-representation, namely the $1$-representation and the $0$-representation, respectively. The $1$-representation of Eq.~\ref{eq:FIM1} is called the logarithmic representation, and the  $0$-representation of Eq.~\ref{eq:FIM0} 
is called the square root representation.

Note that $I_{ij}(\theta)=E[\partial_i l\partial_j l]=\int p\partial_i l\partial_j l\dnu(x)=\int \partial_i p\partial_j l\dnu(x)=I_{ij}^{(1)}(\theta)$ since $\partial_i l=\frac{\partial_i p}{p}$

In information geometry~\cite{IG-2014}, $\{\partial_i l^{(\alpha)}\}_i$ plays the role of tangent vectors, the {\em $\alpha$-scores}.
Geometrically speaking, the tangent plane $T_{p(x;\theta)}$ can be described using any $\alpha$-base. 
The statistical manifold $M=\{p(x;\theta)\}_\theta$ is imbedded into the function space $\bbR^{\calX}$, where $\calX$ denotes the support of the densities.

Under regular conditions~\cite{CR-2013,IG-2014}, the $\alpha$-representation of the FIM for $\alpha\not=-1$ can further be rewritten as
\begin{equation}
I_{ij}^{(\alpha)}(\theta)=  -\frac{2}{1+\alpha} \int p(x;\theta)^{\frac{1+\alpha}{2}} \partial_i\partial_j l^{(\alpha)}(x;\theta)\dnu(x).
\end{equation}

Since we have 
$$
 \partial_i\partial_j l^{(\alpha)}(x;\theta) = p^{\frac{1-\alpha}{2}}\left(
\partial_i\partial_j l + \frac{1-\alpha}{2} \partial_i l\partial_j l
 \right),
$$
it follows that 
$$
I_{ij}^{(\alpha)}(\theta)= -\frac{2}{1+\alpha} \left(-I_{ij}(\theta)+ \frac{1-\alpha}{2} I_{ij}\right)=I_{ij}(\theta).
$$


Notice that when $\alpha=1$, we recover the equivalent expression of the FIM (under mild conditions)
$$
I_{ij}^{(1)}(\theta)= -E[\nabla^2 \log p(x;\theta)].
$$

In particular, when the family is an exponential family~\cite{EF-2009} with cumulant function $F(\theta)$, we have
$$
I(\theta)=\nabla^2 F(\theta)\succ 0.
$$

Similarly, the coefficients of the $\alpha$-connection can be expressed using the $\alpha$-representation as
$$
\Gamma_{ij,k}^{(\alpha)}= \int \partial_i\partial_j l^{(\alpha)}\partial_k^{(-\alpha)} \dnu(x).
$$

The Riemannian metric tensor $g_{ij}$ (a geometric object) can be expressed 
in matrix form $I_{ij}^{(\alpha)}(\theta)$ using the $\alpha$-base, and this tensor is called the Fisher metric tensor.

The FIM may further be represented using the more general $(\rho,\tau)$-monotone embeddings~\cite{naudts2018rho}:
Let $\rho$ and $\tau$ be two strictly increasing functions, and $f$ a strictly convex function such that $f'(\rho(u))=\tau(u)$ (with $f^*$ denoting its convex conjugate).
Let us write $p_\theta(x)=p(x;\theta)$. Then we have $\leftsup{\rho,\tau}g(\theta)=[\leftsup{\rho,\tau}g_{ij}(\theta)]_{ij}$ with
\begin{eqnarray}
\leftsup{\rho,\tau}g_{ij}(\theta) &=& \int \left(\partial_i\rho(p_\theta(x))\right)   \left(\partial_j\tau(p_\theta(x))\right) \dnu(x),\\
&=& \int f''(\rho(p_\theta(x)))  \left(\partial_i\rho(p_\theta(x))\right) \left(\partial_j\rho(p_\theta(x))\right)\dnu(x),\\
&=& \int (f^*)''(\tau(p_\theta(x))) \left(\partial_i\tau(p_\theta(x))\right) \left(\partial_j\tau(p_\theta(x))\right) \dnu(x),\\
&=& \int \frac{1}{\rho'(p_\theta(x)) \tau'(p_\theta(x)) }  \left(\partial_i p_\theta(x) \right) \left(\partial_j p_\theta(x) \right)\dnu(x).
\end{eqnarray} 

This last equation shows that there is a gauge function freedom $\Psi(u):=\frac{1}{\rho'(u) \tau'(u) }$ when calculating the FIM.

\vskip 1cm
Initially created 19th September 2017 (last updated \today).

\bibliographystyle{plain}
\bibliography{alphaFIMBib}
\end{document}


 