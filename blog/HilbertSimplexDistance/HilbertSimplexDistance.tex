\documentclass{article}
\usepackage{fullpage,amssymb,amsmath,url}
\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\square$}
\newtheorem{theorem}{Theorem}
\def\bbN{\mathbb{N}}
\def\calC{\mathcal{C}}
\def\calP{\mathcal{P}}
\def\calQ{\mathcal{Q}}
\def\calF{\mathcal{F}}
\def\bbR{\mathbb{R}}
\def\bbF{\mathbb{F}}
\def\SEB{\mathrm{SEB}}
\def\ceil#1{\lceil #1\rceil}
\def\eps{\epsilon}
\def\inner#1#2{\langle #1,#2\rangle}
\def\KL{\mathrm{KL}}
\def\bbR{\mathbb{R}}
\def\calX{\mathcal{X}}
\def\Hilbert{\mathrm{Hilbert}}

%\title{Kernelized Smallest Enclosing Ball  Coresets for  Support Vector Clustering}

\title{Hilbert's simplex distance:\\ A non-separable information monotone distance}

\author{Frank Nielsen\\ {\tt Frank.Nielsen@acm.org}}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
This note shows that the Hilbert's metric distance in the probability simplex is a non-separable distance which satisfies the information monotonicity. 
\end{abstract}

Consider the open cone $\bbR_{++}^d$ of positive measures (i.e., histograms with $d$ positive bins) with its open probability simplex subset 
$\Delta_d=\{(x_1,\ldots,x_d)\in\bbR_+^d\ :\ \sum_{i=1}^d  x_i=1\}$.

The $f$-divergence~\cite{amari2016information} between $p,q\in\Delta_d$ is defined for a convex function $f(u)$ such that $f(1)=0$ and $f(u)$ strictly convex at $1$ by:
$$
I_f[p:q]:=\sum_{i=1}^d p[i] f(q[i]/p[i])\geq 0.
$$
For example, the Kullback-Leibler divergence is a $f$-divergence for $f(u)=-\log u$.

All $f$-divergences are separable by construction: 
That is, they can be expressed as sum of coordinate-wise scalar divergences: Here, $I_f[p:q]:=\sum_{i=1}^d i_{f}(p[i]:q[i])$, where $i_f$ is a scalar $f$-divergence.
Moreover, $f$-divergences are information monotone: 
That is, let $\calX=\{X_1,\ldots, X_m\}$ be a partition of $\{1,\ldots, n\}$ into $m\leq n$ pairwise disjoint subsets $X_i$'s.
For $p\in\Delta_n$, let $p_{|\calX}\in\Delta_m$ denote the induced probability mass function with $p_{|\calX}(i)=\sum_{j\in X_i} p(i)$.
Then we have
$$
I_f[p_{|\calX}:q_{|\calX}]\leq I_f[p:q], \quad\forall\calX
$$ 
Moreoever, it can be shown that the only separable divergences satisfying this partition inequality are $f$-divergences~\cite{amari2016information} when $n>2$. The special curious binary case $n=2$ is dealt in~\cite{jiao2014information}.

Now, consider the non-separable Hilbert distance in the probability simplex~\cite{nielsen2019clustering}:
$$
D_\Hilbert[p,q]=\log \frac{\max_i \frac{p_i}{q_i}}{\min_i \frac{p_i}{q_i}}. 
$$
This dissimilarity measure is a projective distance on $\bbR^d_{++}$ (Hilbert's projective distance) because 
we have $D_\Hilbert[\lambda p,\lambda' q]=D_\Hilbert[p,q]$ for any $\lambda,\lambda'>0$.
However, the Hilbert distance is a metric distance on $\Delta_d$.

We state the main theorem:

\begin{theorem}
The Hilbert distance on the probability simplex is an information monotone non-separable distance.
\end{theorem}

\begin{proof}
We can represent the coarse-graining mapping $p\mapsto p_{|\calX}$ by a linear application with a $m\times n$ matrix $A$ with columns  summing  up to one (i.e., positive column-stochastic matrix):
$$
p_{|\calX} = A \times p.
$$

For example, the partition $\calX=\{X_1=\{1,2\},X_2=\{3,4\}\}$ (with $n=4$ and $m=2$) is represented by the matrix
$$
A=\left[\begin{array}{llll}1&1&0&0\cr 0&0&1&1\end{array}\right].
$$

Now, a key property of Hilbert distance is Birkhoff's contraction mapping theorem~\cite{birkhoff1957extensions,bushell1973projective}:
$$
D_\Hilbert[Ap,Aq]\leq \tanh\left(\frac{1}{4}\Delta(A)\right)\, D_\Hilbert[p,q],
$$
where $\Delta(A)$ is called the projective diameter of the positive mapping $A$:
$$
\Delta(A)=\sup\{D_\Hilbert[Ap,Aq]\ :\ p,q\in\bbR_{++}^d\}.
$$


Since $0\leq \tanh(x)\leq 1$ for $x\geq 0$, we get the property that  Hilbert distance on the probability simplex is an information monotone non-separable distance:
$$
D_\Hilbert[p_{|\calX},q_{|\calX}]\leq D_\Hilbert[p,q]
$$
\end{proof}

Another example of non-separable information monotone distance is Aitchison's distance on the probability simplex~\cite{erb2021information} (using for compositional data analysis).

\bibliographystyle{plain}
\bibliography{HilbertsimplexBib}

\end{document}
