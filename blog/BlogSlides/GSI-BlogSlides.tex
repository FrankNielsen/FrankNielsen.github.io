\documentclass{beamer}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsthm}
\usepackage{xcolor}
\usepackage{comment}
\definecolor{red}{RGB}{255,0,0}

 

\setbeamertemplate{bibliography item}{\insertbiblabel}

\def\myframepart#1{\begin{frame}\begin{center}{\Huge \bf #1}\end{center}\end{frame}}
  
\DeclareMathOperator*{\argmin}{arg\,min}

\setbeamersize{text margin left=0pt,text margin right=0pt}


\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
%\newtheorem{problem}{Problem}
%\newtheorem{fact}{Fact}

\def\textred#1{#1}
\def\textblue#1{#1}
 \def\map{\mathrm{MAP}}
\def\calX{\mathcal{X}}
\def\dmu{\mathrm{d}\mu}

\usetheme{default}
\usefonttheme{structurebold}
\setbeamercovered{invisible}
\setbeamertemplate{navigation symbols}{} 

\usepackage{graphicx}
\graphicspath{{./figs/}}
 
 

\begin{document}

\title{Geometric Sciences of Information:\\ Random musings}
 
\date{Last updated, \today}

\author{Frank Nielsen\\ {\tt Frank.Nielsen@acm.org}}
 
\begin{frame}
\titlepage
\end{frame}

\begin{comment}
Updated https://arxiv.org/abs/2204.10952
- $f$-divergences between densities of a multivariate location family amount to an increasing function of their Mahalanobis distance, multivariate $f$-div = univariate $f$-div (=> fast Monte Carlo estimations)
- $f$-div between densities of a multivariate scale family is a spectral matrix divergence
\end{comment}


\begin{frame}{$f$-divergences between location-scale families}

\begin{itemize}
	\item Multivariate 
	location-scale family with standard density $p(x)$:
	\textcolor{red}{$
p_{\mu, \Sigma}(x) := |\Sigma|^{-1/2} \,  p\left(\Sigma^{-1/2}(x-\mu)\right), x \in \mathbb{R}^d, \textcolor{blue}{p(x)=\tilde{p}(\|x^2\|)}
$}   (include multivariate normal distributions, multivariate Cauchy distributions) 
\item $f$-divergence between $P,Q\ll\mu$ for strictly convex generator $f\in C^2$ with $f(1)=0$:
$
I_f(P:Q):=\int_\calX p\, f\left(\frac{q}{p}\right) \dmu
$ (include KL divergence)

\item  
$\textcolor{red}{I_f \left(p_{\mu_1, \Sigma} : p_{\mu_2, \Sigma} \right)= h_f \left( \Delta_\Sigma^2(\mu_1,\mu_2) \right)}$ where
\textcolor{blue}{$\Delta_\Sigma^2(\mu_1,\mu_2) := (\mu_2-\mu_1)^\top\, \Sigma^{-1} (\mu_2-\mu_1)$} for strictly increasing $h_f$ (Mahalanobis distance $\Delta_\Sigma$)\\
Preserve relative comparisons:\\
$I_f(p_{\mu_1, \Sigma} : p_{\mu_2, \Sigma})<I_f(p_{\mu_1', \Sigma} : p_{\mu_2', \Sigma}) \Leftrightarrow 
\Delta_\Sigma^2(\mu_1,\mu_2) < \Delta_\Sigma^2(\mu_1',\mu_2')
$ \\
{\footnotesize $\Rightarrow$ Voronoi diagrams, minimum enclosing ball, $k$-center clustering {\bf independent} of $f$}

\item Since $\Delta_\Sigma^2(\mu_1,\mu_2)=\Delta_{1}(0,\Delta_\Sigma^2(\mu_1,\mu_2))$, multivariate $f$-div. amounts to univariate $f$-div ($\rightarrow$ fast Monte Carlo estimations: eg., Jensen-Shannon div.):
\textcolor{red}{$I_f[p_{\mu_1,\Sigma},p_{\mu_2,\Sigma}] = I_f \left[p_{0,1},p_{\Delta_\Sigma(\mu_1,\mu_2),1}\right]$}

\item Spectral matrix $f$-divergences for multivariate scale families:
\textcolor{red}{$I_f[p_{\mu,\Sigma_1}:p_{\mu,\Sigma_2}]=E_f(|1-\lambda_1|,\ldots,|1-\lambda_d|)$}, 
where $E_f(\cdot)$ is a $d$-variate totally symmetric function and $\lambda_i\in \mathrm{sp}(\Sigma_2\Sigma_1^{-1})$. 
\hfill
{\bf \textcolor{green}{[arXiv:2204.10952]}}
\end{itemize}


\end{frame}

\end{document}

%%%

\begin{frame}{Aitchison distance vs Hilbert simplex distance}

\begin{itemize}
	\item Aitchison distance:
	
\begin{eqnarray*}
\rho_{\mathrm{Aitchison}}(p,q) &=& \sqrt{\sum_{i=1}^d \left( \log \frac{p^i}{G(p)}-\log \frac{q^i}{G(q)} \right)^2}\\
&=& \left\|\log\frac{p}{G(p)}-\log\frac{q}{G(p)}\right\|_2
\end{eqnarray*}
where $G(p)=\left(\prod_{i=1}^d p^i\right)^{\frac{1}{d}}=\exp\left(\frac{1}{d}\sum_{i=1}^d \log p^i\right)$ is the geometric mean of coordinates

\item Hilbert simplex distance: 
\begin{eqnarray*}
\rho_{\mathrm{Hilbert}}(p,q) &=&  \log \frac{\max _{i\in\{1,\ldots, d\}} \frac{p_{i}}{q_{i}}}{\min _{j\in\{1,\ldots, d\}} \frac{p_{j}}{q_{j}}}\\
&=& \left\|\log\frac{p}{G(p)}-\log\frac{q}{G(q)} \right\|_{\mathrm{var}}
\end{eqnarray*}
where $\|\cdot\|_{\mathrm{var}}$ denotes the variation norm: $\|x\|_{\mathrm{var}}:= (\max_i x_i)-(\min_i x_i)$
\end{itemize}
\end{frame}


\begin{frame}{Geometry of Hamming spaces}
\end{frame}


\begin{frame}[allowframebreaks]
        \frametitle{References}
				\tiny
\bibliographystyle{plain}
\bibliography{RandomMusings-Bib}
\end{frame}

\end{document}
