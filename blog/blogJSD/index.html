<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Jensen-Shannon divergence and diversity index:
Origins and some extensions</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="JensenShannonDiv.tex"> 
<link rel="stylesheet" type="text/css" href="JensenShannonDiv.css"> 
</head><body 
>
   <div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead">Jensen-Shannon divergence and diversity index:<br />
Origins and some extensions</h2>
              <div class="author" ><span 
class="cmr-12">Frank Nielsen</span>
<br /><span 
class="cmr-12">Sony Computer Science Laboratories Inc.</span>
<br />              <span 
class="cmr-12">Tokyo, Japan</span></div><br />
<div class="date" ><span 
class="cmr-12">April 2021</span></div>
   </div>
   <div 
class="abstract" 
>
   <h3 class="abstracttitle">
<span 
class="cmbx-10">Abstract</span>
</h3>
     <!--l. 30--><p class="noindent" ><span 
class="cmr-10">Lin coined the skewed Jensen-Shannon divergence between two distributions in 1991, and further</span>
     <span 
class="cmr-10">extended  it  to  the  Jensen-Shannon  diversity  of  a  set  of  distributions.  Sibson  proposed  the</span>
     <span 
class="cmr-10">information radius based on R</span><span 
class="cmr-10">ényi </span><span 
class="cmmi-10">&#x03B1;</span><span 
class="cmr-10">-entropies in 1969, and recovered for the special case of</span>
     <span 
class="cmmi-10">&#x03B1; </span><span 
class="cmr-10">= 1 the Jensen-Shannon diversity index. In this note, we summarize how the Jensen-Shannon</span>
     <span 
class="cmr-10">divergence and diversity index were extended by either considering skewing vectors or using</span>
     <span 
class="cmr-10">mixtures induced by generic means.</span>
</div>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Origins</h3>
<!--l. 38--><p class="noindent" >Let (<span 
class="cmsy-10x-x-109"><img 
src="cmsy10-58.png" alt="X" class="10-109x-x-58" /></span><span 
class="cmmi-10x-x-109">,</span><span 
class="cmsy-10x-x-109"><img 
src="cmsy10-46.png" alt="F" class="10-109x-x-46" /></span><span 
class="cmmi-10x-x-109">,&#x03BC;</span>) be a measure space, and (<span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,P</span><sub><span 
class="cmr-8">1</span></sub>)<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,</span>(<span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">n</span></sub><span 
class="cmmi-10x-x-109">,P</span><sub><span 
class="cmmi-8">n</span></sub>) be <span 
class="cmmi-10x-x-109">n </span>weighted probability measures dominated
by a measure <span 
class="cmmi-10x-x-109">&#x03BC; </span>(with <span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">i</span></sub> <span 
class="cmmi-10x-x-109">&#x003E; </span>0 and <span 
class="cmex-10">&#x2211;</span>
  <span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">i</span></sub> = 1). Denote by <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /> </span>:= <span 
class="cmsy-10x-x-109">{</span>(<span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,p</span><sub><span 
class="cmr-8">1</span></sub>)<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,</span>(<span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">n</span></sub><span 
class="cmmi-10x-x-109">,p</span><sub><span 
class="cmmi-8">n</span></sub>)<span 
class="cmsy-10x-x-109">} </span>the set of their
weighted Radon-Nikodym densities <span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">i</span></sub> = <img 
src="JensenShannonDiv0x.png" alt="ddP&#x03BC;i"  class="frac" align="middle"> with respect to <span 
class="cmmi-10x-x-109">&#x03BC;</span>.
<!--l. 42--><p class="indent" >   A <span 
class="cmti-10x-x-109">statistical divergence </span><span 
class="cmmi-10x-x-109">D</span>[<span 
class="cmmi-10x-x-109">p </span>: <span 
class="cmmi-10x-x-109">q</span>] is a measure of dissimilarity between two densities <span 
class="cmmi-10x-x-109">p </span>and <span 
class="cmmi-10x-x-109">q </span>(i.e., a
2-point distance) such that <span 
class="cmmi-10x-x-109">D</span>[<span 
class="cmmi-10x-x-109">p </span>: <span 
class="cmmi-10x-x-109">q</span>] <span 
class="cmsy-10x-x-109">&#x2265; </span>0 with equality if and only if <span 
class="cmmi-10x-x-109">p</span>(<span 
class="cmmi-10x-x-109">x</span>) = <span 
class="cmmi-10x-x-109">q</span>(<span 
class="cmmi-10x-x-109">x</span>) <span 
class="cmmi-10x-x-109">&#x03BC;</span>-almost everywhere. A
<span 
class="cmti-10x-x-109">statistical diversity index </span><span 
class="cmmi-10x-x-109">D</span>(<span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /></span>) is a measure of variation of the weighted densities in <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /> </span>related to a
measure of centrality, i.e., a <span 
class="cmmi-10x-x-109">n</span>-point distance which generalizes the notion of 2-point distance when
<span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /></span><sub><span 
class="cmr-8">2</span></sub>(<span 
class="cmmi-10x-x-109">p,q</span>) := <span 
class="cmsy-10x-x-109">{</span>(<img 
src="JensenShannonDiv1x.png" alt="1
2"  class="frac" align="middle"><span 
class="cmmi-10x-x-109">,p</span><sub><span 
class="cmr-8">1</span></sub>)<span 
class="cmmi-10x-x-109">,</span>(<img 
src="JensenShannonDiv2x.png" alt="1
2"  class="frac" align="middle"><span 
class="cmmi-10x-x-109">,p</span><sub><span 
class="cmr-8">2</span></sub>)<span 
class="cmsy-10x-x-109">}</span>:
                                                                                         
                                                                                         
   <div class="math-display" >
<img 
src="JensenShannonDiv3x.png" alt="D [p : q] := D (P2(p,q)).
" class="math-display" ></div>
<!--l. 48--><p class="indent" >   The fundamental measure of dissimilarity in information theory is the <span 
class="cmmi-10x-x-109">I</span><span 
class="cmti-10x-x-109">-divergence </span>(also called the
<span 
class="cmti-10x-x-109">Kullback-Leibler divergence</span>, KLD, see Equation&#x00A0;(2.5) page 5&#x00A0;of&#x00A0;[<a 
href="#XKullback-1997">5</a>]):
   <div class="math-display" >
<img 
src="JensenShannonDiv4x.png" alt="            &#x222B;         ( p(x))
DKL [p : q] :=  p(x)log  ----  d&#x03BC;(x).
             X          q(x)
" class="math-display" ></div>
<!--l. 53--><p class="indent" >   The KLD is asymmetric (hence the delimiter notation &#8220;:&#8221; instead of &#8216;,&#8217;) but can be symmetrized by
defining the Jeffreys <span 
class="cmmi-10x-x-109">J</span><span 
class="cmti-10x-x-109">-divergence </span>(Jeffreys divergence, denoted by <span 
class="cmmi-10x-x-109">I</span><sub><span 
class="cmr-8">2</span></sub> in Equation (1) in 1946&#8217;s
paper&#x00A0;[<a 
href="#XJeffreys-1946">4</a>]):
   <div class="math-display" >
<img 
src="JensenShannonDiv5x.png" alt="                                  &#x222B;                 (p(x))
DJ [p,q] := DKL [p : q]+ DKL [q : p] = (p(x) - q(x )) log q(x)  d&#x03BC; (x ).
                                   X
" class="math-display" ></div>
<!--l. 57--><p class="indent" >   Although symmetric, any positive power of Jeffreys divergence fails to satisfy the triangle inequality:
That is, <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmmi-8">J</span></sub><sup><span 
class="cmmi-8">&#x03B1;</span></sup> is never a metric distance for any <span 
class="cmmi-10x-x-109">&#x03B1; &#x003E; </span>0, and furthermore <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmmi-8">J</span></sub><sup><span 
class="cmmi-8">&#x03B1;</span></sup> cannot be upper
bounded.
<!--l. 60--><p class="indent" >   In 1991, Lin proposed the asymmetric <span 
class="cmmi-10x-x-109">K</span><span 
class="cmti-10x-x-109">-divergence </span>(Equation (3.2) in&#x00A0;[<a 
href="#XJS-1991">7</a>]):
   <div class="math-display" >
                                                                                         
                                                                                         
<img 
src="JensenShannonDiv6x.png" alt="                [        ]
D  [p : q] := D  p : p-+-q ,
  K          KL       2
" class="math-display" ></div>
<!--l. 64--><p class="indent" >   and defined the <span 
class="cmmi-10x-x-109">L</span><span 
class="cmti-10x-x-109">-divergence </span>by analogy to Jeffreys&#8217;s symmetrization of the KLD (Equation (3.4)
in&#x00A0;[<a 
href="#XJS-1991">7</a>]):
   <div class="math-display" >
<img 
src="JensenShannonDiv7x.png" alt="DL [p,q] = DK [p : q]+ DK [q : p].
" class="math-display" ></div>
<!--l. 69--><p class="indent" >   By noticing that
   <div class="math-display" >
<img 
src="JensenShannonDiv8x.png" alt="            [     ]
             p-+-q
DL [p,q] = 2h   2    - (h[p] + h[q]),
" class="math-display" ></div>
<!--l. 73--><p class="indent" >   where <span 
class="cmmi-10x-x-109">h </span>denotes Shannon entropy (Equation (3.14) in&#x00A0;[<a 
href="#XJS-1991">7</a>]), Lin coined the (skewed) <span 
class="cmti-10x-x-109">Jensen-Shannon</span>
<span 
class="cmti-10x-x-109">divergence </span>between two weighted densities (1 <span 
class="cmsy-10x-x-109">- </span><span 
class="cmmi-10x-x-109">&#x03B1;,p</span>) and (<span 
class="cmmi-10x-x-109">&#x03B1;,q</span>) for <span 
class="cmmi-10x-x-109">&#x03B1; </span><span 
class="cmsy-10x-x-109">&#x2208; </span>(0<span 
class="cmmi-10x-x-109">,</span>1) as follows (Equation (4.1)
in&#x00A0;[<a 
href="#XJS-1991">7</a>]):
   <table 
class="equation"><tr><td>
   <div class="math-display" >
<img 
src="JensenShannonDiv9x.png" alt="DJS,&#x03B1;[p,q] = h[(1- &#x03B1;)p + &#x03B1;q]- (1 - &#x03B1;)h[p]- &#x03B1;h[q].
" class="math-display" ><a 
 id="x1-1001r1"></a></div>
   </td><td class="equation-label">(1)</td></tr></table>
                                                                                         
                                                                                         
<!--l. 76--><p class="nopar" >
<!--l. 78--><p class="indent" >   Finally, Lin defined the <span 
class="cmti-10x-x-109">generalized Jensen-Shannon divergence </span>(Equation (5.1) in&#x00A0;[<a 
href="#XJS-1991">7</a>]) for a finite
weighted set of densities:
   <div class="math-display" >
<img 
src="JensenShannonDiv10x.png" alt="           [       ]
D   [P] = h &#x2211;  w p   - &#x2211;  w h[p ].
  JS         i   i i    i  i   i
" class="math-display" ></div>
<!--l. 82--><p class="indent" >   This generalized Jensen-Shannon divergence is nowadays called the <span 
class="cmti-10x-x-109">Jensen-Shannon diversity</span>
<span 
class="cmti-10x-x-109">index</span>.
<!--l. 84--><p class="indent" >   To contrast with the Jeffreys&#8217; divergence, the Jensen-Shannon divergence (JSD) <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span></sub> := <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span><span 
class="cmmi-8">,</span><img 
src="JensenShannonDiv11x.png" alt="12"  class="frac" align="middle"></sub> is upper
bounded by log 2 (does not require the densities to have the same support), and <img 
src="JensenShannonDiv12x.png" alt="&#x221A; ----
  DJS"  class="sqrt" > is a metric
distance&#x00A0;[<a 
href="#XJSmetric-2003">2</a>,&#x00A0;<a 
href="#XJSmetric-2004">3</a>]. Lin cited precursor work&#x00A0;[<a 
href="#XWongYOU-1985">17</a>,&#x00A0;<a 
href="#XJW-1988">8</a>] yielding definition of the Jensen-Shannon divergence:
The Jensen-Shannon divergence of Eq.&#x00A0;<a 
href="#x1-1001r1">1<!--tex4ht:ref: eq:JSh --></a> is the so-called &#8220;increments of entropy&#8221; defined in (19) and (20)
of&#x00A0;[<a 
href="#XWongYOU-1985">17</a>].
<!--l. 89--><p class="indent" >   The Jensen-Shannon diversity index was also obtained very differently by Sibson in 1969 when he
defined the <span 
class="cmti-10x-x-109">information radius</span>&#x00A0;[<a 
href="#XSibson-1969">16</a>] of order <span 
class="cmmi-10x-x-109">&#x03B1; </span>using Rényi <span 
class="cmmi-10x-x-109">&#x03B1;</span>-means and Rényi <span 
class="cmmi-10x-x-109">&#x03B1;</span>-entropies&#x00A0;[<a 
href="#XRenyi-1961">15</a>]. In
particular, the information radius IR<sub><span 
class="cmr-8">1</span></sub> of order 1 of a weighted set <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /> </span>of densities is a diversity index
obtained by solving the following variational optimization problem:
   <table 
class="equation"><tr><td>
   <div class="math-display" >
<img 
src="JensenShannonDiv13x.png" alt="             &#x2211;n
IR1[P ] := min    wiDKL [pi : c].
           c  i=1
" class="math-display" ><a 
 id="x1-1002r2"></a></div>
   </td><td class="equation-label">(2)</td></tr></table>
<!--l. 93--><p class="nopar" >
<!--l. 95--><p class="indent" >   Sibson solved a more general optimization problem, and obtained the following expression (term <span 
class="cmmi-10x-x-109">K</span><sub><span 
class="cmr-8">1</span></sub> in
Corollary 2.3&#x00A0;[<a 
href="#XSibson-1969">16</a>]):
                                                                                         
                                                                                         
   <div class="math-display" >
<img 
src="JensenShannonDiv14x.png" alt="          [       ]
IR [P] = h &#x2211;  w p   - &#x2211;  w h[p ] := D  [P].
  1         i   ii     i  i   i     JS
" class="math-display" ></div>
<!--l. 99--><p class="indent" >   Thus Eq.&#x00A0;<a 
href="#x1-1002r2">2<!--tex4ht:ref: eq:Sibson --></a> is a variational definition of the Jensen-Shannon divergence.
<!--l. 102--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-20002"></a>Some extensions</h3>
     <ul class="itemize1">
     <li class="itemize"><span 
class="cmbx-10x-x-109">Skewing the JSD.</span>
     <!--l. 109--><p class="noindent" >The <span 
class="cmmi-10x-x-109">K</span>-divergence of Lin can be skewed with a scalar parameter <span 
class="cmmi-10x-x-109">&#x03B1; </span><span 
class="cmsy-10x-x-109">&#x2208; </span>(0<span 
class="cmmi-10x-x-109">,</span>1) to give
     <table 
class="equation"><tr><td>
<div class="math-display" >
<img 
src="JensenShannonDiv15x.png" alt="DK, &#x03B1;[p : q] := DKL [p : (1- &#x03B1; )p+ &#x03B1;q].
" class="math-display" ><a 
 id="x1-2001r3"></a></div>
     </td><td class="equation-label">(3)</td></tr></table>
     <!--l. 112--><p class="nopar" >
     Skewing parameter <span 
class="cmmi-10x-x-109">&#x03B1; </span>was first studied in&#x00A0;[<a 
href="#XskewJS-2001">6</a>] (2001, see Table&#x00A0;2 of&#x00A0;[<a 
href="#XskewJS-2001">6</a>]). We proposed to unify
     the Jeffreys divergence with the Jensen-Shannon divergence as follows (Equation 19
     in&#x00A0;[<a 
href="#Xnielsen2010family">9</a>]):
     <table 
class="equation"><tr><td>
<div class="math-display" >
                                                                                         
                                                                                         
<img 
src="JensenShannonDiv16x.png" alt="DJ  [p : q] := DK,&#x03B1;-[p-: q]+-DK,-&#x03B1;[q-: p].
 K,&#x03B1;                   2
" class="math-display" ><a 
 id="x1-2002r4"></a></div>
     </td><td class="equation-label">(4)</td></tr></table>
     <!--l. 117--><p class="nopar" >
     When <span 
class="cmmi-10x-x-109">&#x03B1; </span>= <img 
src="JensenShannonDiv17x.png" alt="1
2"  class="frac" align="middle">, we have <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmmi-8">K,</span><img 
src="JensenShannonDiv18x.png" alt="12"  class="frac" align="middle"></sub><sup><span 
class="cmmi-8">J</span></sup> = <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span></sub>, and when <span 
class="cmmi-10x-x-109">&#x03B1; </span>= 1, we get <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmmi-8">K,</span><span 
class="cmr-8">1</span></sub><sup><span 
class="cmmi-8">J</span></sup> = <img 
src="JensenShannonDiv19x.png" alt="1
2"  class="frac" align="middle"><span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmmi-8">J</span></sub>.
     <!--l. 120--><p class="noindent" >Notice that
<div class="math-display" >
<img 
src="JensenShannonDiv20x.png" alt="D &#x03B1;,&#x03B2;[p;q] := (1- &#x03B2; )D  [p : (1- &#x03B1; )p + &#x03B1;q]+ &#x03B2;D   [q : (1- &#x03B1;)p+ &#x03B1;q ]
  JS                KL                      KL
" class="math-display" ></div>
     <!--l. 124--><p class="noindent" >amounts to calculate
<div class="math-display" >
<img 
src="JensenShannonDiv21x.png" alt="h&#x00D7;[(1- &#x03B2; )p + &#x03B2;q : (1- &#x03B1; )p+ &#x03B1;q ]- ((1 - &#x03B2;)h[p]+ &#x03B2;h[q])
" class="math-display" ></div>
     <!--l. 128--><p class="noindent" >where
<div class="math-display" >
<img 
src="JensenShannonDiv22x.png" alt="           &#x222B;
h &#x00D7;[p,q] :=   - p(x) log q(x )d &#x03BC;(x)
" class="math-display" ></div>
                                                                                         
                                                                                         
     <!--l. 132--><p class="noindent" >denotes the <span 
class="cmti-10x-x-109">cross-entropy</span>. By choosing <span 
class="cmmi-10x-x-109">&#x03B1; </span>= <span 
class="cmmi-10x-x-109">&#x03B2;</span>, we have <span 
class="cmmi-10x-x-109">h</span><sup><span 
class="cmsy-8">&#x00D7;</span></sup>[(1<span 
class="cmsy-10x-x-109">-</span><span 
class="cmmi-10x-x-109">&#x03B2;</span>)<span 
class="cmmi-10x-x-109">p</span>+<span 
class="cmmi-10x-x-109">&#x03B2;q </span>: (1<span 
class="cmsy-10x-x-109">-</span><span 
class="cmmi-10x-x-109">&#x03B1;</span>)<span 
class="cmmi-10x-x-109">p</span>+<span 
class="cmmi-10x-x-109">&#x03B1;q</span>] = <span 
class="cmmi-10x-x-109">h</span>[(1<span 
class="cmsy-10x-x-109">-</span><span 
class="cmmi-10x-x-109">&#x03B1;</span>)<span 
class="cmmi-10x-x-109">p</span>+<span 
class="cmmi-10x-x-109">&#x03B1;q</span>],
     and thus recover the skewed Jensen-Shannon divergence of Eq.&#x00A0;<a 
href="#x1-1001r1">1<!--tex4ht:ref: eq:JSh --></a>.
     <!--l. 136--><p class="noindent" >In&#x00A0;[<a 
href="#Xnielsen2020generalization">11</a>] (2020), we considered a positive <span 
class="cmti-10x-x-109">skewing vector </span><span 
class="cmmi-10x-x-109">&#x03B1; </span><span 
class="cmsy-10x-x-109">&#x2208; </span>[0<span 
class="cmmi-10x-x-109">,</span>1]<sup><span 
class="cmmi-8">k</span></sup> and a unit positive weight <span 
class="cmmi-10x-x-109">w</span>
     belonging to the standard simplex &#x0394;<sub><span 
class="cmmi-8">k</span></sub>, and defined the following <span 
class="cmti-10x-x-109">vector-skewed Jensen-Shannon</span>
     <span 
class="cmti-10x-x-109">divergence</span>: <div class="eqnarray">
<div class="math-display" >
<img 
src="JensenShannonDiv23x.png" alt="                k
D &#x03B1;,w[p : q] := &#x2211;  DKL [(1- &#x03B1;i)p+ &#x03B1;iq : (1 - ¯&#x03B1;)p+ ¯&#x03B1;q],                (5)
 JS            i=1
                                  k
           =   h[(1- &#x03B1;¯)p+ ¯&#x03B1;q ]- &#x2211;  h[(1 - &#x03B1; )p &#x03B1; q],                (6)
                                 i=1        i  +  i
" class="math-display" ></div>
     <!--l. 140--><p class="noindent" ></div>where <span class="bar-css"><span 
class="cmmi-10x-x-109">&#x03B1;</span></span> = <span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmmi-8">k</span></sup><span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">&#x03B1;</span><sub><span 
class="cmmi-8">i</span></sub>. The divergence <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span></sub><sup><span 
class="cmmi-8">&#x03B1;,w</span></sup> generalizes the (scalar) skew Jensen-Shannon
     divergence when <span 
class="cmmi-10x-x-109">k </span>= 1, and is a Ali-Silvey-Csiszár <span 
class="cmmi-10x-x-109">f</span>-divergence upper bounded by log <img 
src="JensenShannonDiv24x.png" alt="¯&#x03B1;(11- ¯&#x03B1;)"  class="frac" align="middle">&#x00A0;[<a 
href="#Xnielsen2020generalization">11</a>].
     </li>
     <li class="itemize"><span 
class="cmbx-10x-x-109">A priori mid-density</span>. The JSD can be interpreted as the total divergence of the densities to the
     <span 
class="cmti-10x-x-109">mid-density</span> <span class="bar-css"><span 
class="cmmi-10x-x-109">p</span></span> = <span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmmi-8">n</span></sup><span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">i</span></sub>, a statistical mixture:
<div class="math-display" >
<img 
src="JensenShannonDiv25x.png" alt="         &#x2211;n                      &#x2211;n
DJS [P ] =   wiDKL  [pi : ¯p] = h[¯p]-  wih[pi].
         i=1                     i=1
" class="math-display" ></div>
     <!--l. 149--><p class="noindent" >Unfortunately, the JSD between two Gaussian densities is not known in closed form because of the
     definite integral of a log-sum term (i.e., <span 
class="cmmi-10x-x-109">K</span>-divergence between a density and a mixture density <span class="bar-css"><span 
class="cmmi-10x-x-109">p</span></span>).
     For the special case of the Cauchy family, a closed-form formula&#x00A0;[<a 
href="#XCauchyJSD-2021">14</a>] for the JSD between two
     Cauchy densities was obtained. Thus we may choose a <span 
class="cmti-10x-x-109">geometric mixture distribution</span>&#x00A0;[<a 
href="#XJSsym-2019">10</a>] instead of
     the ordinary arithmetic mixture <span class="bar-css"><span 
class="cmmi-10x-x-109">p</span></span>. More generally, we can choose any weighted mean <span 
class="cmmi-10x-x-109">M</span><sub><span 
class="cmmi-8">&#x03B1;</span></sub> (say, the
     geometric mean, or the harmonic mean, or any other power mean) and define a generalization of the
     <span 
class="cmmi-10x-x-109">K</span>-divergence of Equation&#x00A0;<a 
href="#x1-2001r3">3<!--tex4ht:ref: eq:divK --></a>:
                                                                                         
                                                                                         
     <table 
class="equation"><tr><td>
<div class="math-display" >
<img 
src="JensenShannonDiv26x.png" alt="DMK&#x03B1; [p : q] := DK [p : (pq)M &#x03B1;],
" class="math-display" ><a 
 id="x1-2004r7"></a></div>
     </td><td class="equation-label">(7)</td></tr></table>
     <!--l. 154--><p class="nopar" >
     where
<div class="math-display" >
<img 
src="JensenShannonDiv27x.png" alt="             M-&#x03B1;(p(x),q(x))
(pq)M &#x03B1;(x) :=  ZM &#x03B1;(p : q)
" class="math-display" ></div>
     <!--l. 159--><p class="noindent" >is a statistical <span 
class="cmmi-10x-x-109">M</span>-mixture with <span 
class="cmmi-10x-x-109">Z</span><sub><span 
class="cmmi-8">M</span><sub><span 
class="cmmi-6">&#x03B1;</span></sub></sub>(<span 
class="cmmi-10x-x-109">p,q</span>) denoting the normalizing coefficient:
<div class="math-display" >
<img 
src="JensenShannonDiv28x.png" alt="            &#x222B;
ZM &#x03B1;(p : q) =  M &#x03B1;(p (x ),q(x))d&#x03BC;(x)
" class="math-display" ></div>
     <!--l. 164--><p class="noindent" >so that <span 
class="cmex-10">&#x222B;</span>
 (<span 
class="cmmi-10x-x-109">pq</span>)<sub><span 
class="cmmi-8">M</span><sub><span 
class="cmmi-6">&#x03B1;</span></sub></sub>(<span 
class="cmmi-10x-x-109">x</span>)d<span 
class="cmmi-10x-x-109">&#x03BC;</span>(<span 
class="cmmi-10x-x-109">x</span>) = 1. These <span 
class="cmmi-10x-x-109">M</span>-mixtures are well-defined provided the convergence of the
     definite integrals.
     <!--l. 167--><p class="noindent" >Then we define a generalization of the JSD&#x00A0;[<a 
href="#XJSsym-2019">10</a>] termed (<span 
class="cmmi-10x-x-109">M</span><sub><span 
class="cmmi-8">&#x03B1;</span></sub><span 
class="cmmi-10x-x-109">,N</span><sub><span 
class="cmmi-8">&#x03B2;</span></sub>)<span 
class="cmti-10x-x-109">-Jensen-Shannon divergence </span>as
     follows:
     <table 
class="equation"><tr><td>
                                                                                         
                                                                                         
<div class="math-display" >
<img 
src="JensenShannonDiv29x.png" alt="DM &#x03B1;,N&#x03B2;[p : q ] := N &#x03B2; (DK [p : (pq)M ],DK [q : (pq)M ]) ,
  JS                           &#x03B1;              &#x03B1;
" class="math-display" ><a 
 id="x1-2005r8"></a></div>
     </td><td class="equation-label">(8)</td></tr></table>
     <!--l. 170--><p class="nopar" >
     where <span 
class="cmmi-10x-x-109">N</span><sub><span 
class="cmmi-8">&#x03B2;</span></sub> is yet another weighted mean to average the two <span 
class="cmmi-10x-x-109">M</span><sub><span 
class="cmmi-8">&#x03B1;</span></sub>-<span 
class="cmmi-10x-x-109">K</span>-divergences. We have
     <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span></sub> = <span 
class="cmmi-10x-x-109">D</span><sub><span 
class="cmr-8">JS</span></sub><sup><span 
class="cmmi-8">A,A</span></sup> where <span 
class="cmmi-10x-x-109">A</span>(<span 
class="cmmi-10x-x-109">a,b</span>) = <img 
src="JensenShannonDiv30x.png" alt="a+b
-2-"  class="frac" align="middle"> is the arithmetic mean. The geometric JSD yields a closed-form
     formula between two multivariate Gaussians, and has been used in deep learning&#x00A0;[<a 
href="#XVIGJSD-2020">1</a>]. More
     generally, we may consider the Jensen-Shannon symmetrization of an arbitrary distance <span 
class="cmmi-10x-x-109">D</span>
     as
     <table 
class="equation"><tr><td>
<div class="math-display" >
<img 
src="JensenShannonDiv31x.png" alt=" JS
DM &#x03B1;,N &#x03B2;[p : q] := N &#x03B2; (D [p : (pq)M&#x03B1;],D [q : (pq)M&#x03B1;]).
" class="math-display" ><a 
 id="x1-2006r9"></a></div>
     </td><td class="equation-label">(9)</td></tr></table>
     <!--l. 177--><p class="nopar" >
     </li>
     <li class="itemize"><span 
class="cmbx-10x-x-109">A posteriori mid-density</span>. We consider a generalization of Sibson&#8217;s information radius&#x00A0;[<a 
href="#XSibson-1969">16</a>]. Let
     <span 
class="cmmi-10x-x-109">S</span><sub><span 
class="cmmi-8">w</span></sub>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,a</span><sub><span 
class="cmmi-8">n</span></sub>) denote a generic weighted mean of <span 
class="cmmi-10x-x-109">n </span>positive scalars <span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,a</span><sub><span 
class="cmmi-8">n</span></sub>, with weight
     vector <span 
class="cmmi-10x-x-109">w </span><span 
class="cmsy-10x-x-109">&#x2208; </span>&#x0394;<sub><span 
class="cmmi-8">n</span></sub>. Then we define the <span 
class="cmmi-10x-x-109">S</span><span 
class="cmti-10x-x-109">-variational Jensen-Shannon diversity index</span>&#x00A0;[<a 
href="#XvJSD-2021">12</a>]
     as
     <table 
class="equation"><tr><td>
<div class="math-display" >
                                                                                         
                                                                                         
<img 
src="JensenShannonDiv32x.png" alt="DSwvJS(P ) := min Sw (DKL [p1 : c],DKL [pn : c]) .
            c
" class="math-display" ><a 
 id="x1-2007r10"></a></div>
     </td><td class="equation-label">(10)</td></tr></table>
     <!--l. 186--><p class="nopar" >
     When <span 
class="cmmi-10x-x-109">S</span><sub><span 
class="cmmi-8">w</span></sub> = <span 
class="cmmi-10x-x-109">A</span><sub><span 
class="cmmi-8">w</span></sub> (with <span 
class="cmmi-10x-x-109">A</span><sub><span 
class="cmmi-8">w</span></sub>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">&#x2026;</span><span 
class="cmmi-10x-x-109">,a</span><sub><span 
class="cmmi-8">n</span></sub>) = <span 
class="cmex-10">&#x2211;</span>
  <sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmmi-8">n</span></sup><span 
class="cmmi-10x-x-109">w</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">i</span></sub> the arithmetic weighted mean), we recover the
     ordinary Jensen-Shannon diversity index. More generally, we define the <span 
class="cmmi-10x-x-109">S</span><span 
class="cmti-10x-x-109">-Jensen-Shannon index of</span>
     <span 
class="cmti-10x-x-109">an arbitrary distance </span><span 
class="cmmi-10x-x-109">D </span>as
     <table 
class="equation"><tr><td>
<div class="math-display" >
<img 
src="JensenShannonDiv33x.png" alt="DvJS (P) :=  minS  (D [p  : c],...,D [p : c]).
  Sw         c   w    1            n
" class="math-display" ><a 
 id="x1-2008r11"></a></div>
     </td><td class="equation-label">(11)</td></tr></table>
     <!--l. 191--><p class="nopar" >
     When <span 
class="cmmi-10x-x-109">n </span>= 2, this yields a Jensen-Shannon-symmetrization of distance <span 
class="cmmi-10x-x-109">D</span>.
     <!--l. 194--><p class="noindent" >The variational optimization defining the JSD can also be constrained to a (parametric) family of
     densities <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-44.png" alt="D" class="10-109x-x-44" /></span>, thus defining the (<span 
class="cmmi-10x-x-109">S,</span><span 
class="cmsy-10x-x-109"><img 
src="cmsy10-44.png" alt="D" class="10-109x-x-44" /></span>)<span 
class="cmti-10x-x-109">-relative Jensen-Shannon diversity index</span>:
     <table 
class="equation"><tr><td>
<div class="math-display" >
<img 
src="JensenShannonDiv34x.png" alt="  S ,D
D vwJS (P ) := min Sw (DKL [p1 : c],...,DKL [pn : c]).
             c&#x2208;D
" class="math-display" ><a 
 id="x1-2009r12"></a></div>
     </td><td class="equation-label">(12)</td></tr></table>
     <!--l. 198--><p class="nopar" >
                                                                                         
                                                                                         
     <!--l. 201--><p class="noindent" >The relative Jensen-Shannon divergences are useful for clustering applications: Let <span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">&#x03B8;</span><sub><span 
class="cmr-6">1</span></sub></sub> and <span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">&#x03B8;</span><sub><span 
class="cmr-6">2</span></sub></sub> be
     two densities of an exponential family <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-45.png" alt="E" class="10-109x-x-45" /> </span>with cumulant function <span 
class="cmmi-10x-x-109">F</span>(<span 
class="cmmi-10x-x-109">&#x03B8;</span>). Then the <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-45.png" alt="E" class="10-109x-x-45" /></span>-relative
     Jensen-Shannon divergence is the Bregman information of <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-50.png" alt="P" class="10-109x-x-50" /></span><sub><span 
class="cmr-8">2</span></sub>(<span 
class="cmmi-10x-x-109">p,q</span>) for the conjugate function
     <span 
class="cmmi-10x-x-109">F</span><sup><span 
class="cmsy-8">*</span></sup>(<span 
class="cmmi-10x-x-109">&#x03B7;</span>) = <span 
class="cmsy-10x-x-109">-</span><span 
class="cmmi-10x-x-109">h</span>[<span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">&#x03B8;</span></sub>] (with <span 
class="cmmi-10x-x-109">&#x03B7; </span>= <span 
class="cmsy-10x-x-109">&#x2207;</span><span 
class="cmmi-10x-x-109">F</span>(<span 
class="cmmi-10x-x-109">&#x03B8;</span>)). The <span 
class="cmsy-10x-x-109"><img 
src="cmsy10-45.png" alt="E" class="10-109x-x-45" /></span>-relative JSD amounts to a <span 
class="cmti-10x-x-109">Jensen divergence </span>for
     <span 
class="cmmi-10x-x-109">F</span><sup><span 
class="cmsy-8">*</span></sup>:
     <!--l. 206--><p class="noindent" ><div class="eqnarray">
<div class="math-display" >
<img 
src="JensenShannonDiv35x.png" alt="                      1
DvJS [p&#x03B8;1,p&#x03B8;2] =   min --{DKL [p &#x03B8;1 : p&#x03B8;]+ DKL [p&#x03B8;2 : p&#x03B8;]},             (13)
                   &#x03B8;  2
              =   min 1-{BF [&#x03B8; : &#x03B8;1]+ BF [&#x03B8; : &#x03B8;2]},                  (14)
                   &#x03B8;  2
                      1-    *           *
              =   mi&#x03B7;n 2 {BF  [&#x03B7;1 : &#x03B7;]+ BF [&#x03B7;2 : &#x03B7;]},                  (15)
                  F *(&#x03B7;1)+ F *(&#x03B7;2)    *  *
              =   -------2------- - F (&#x03B7; ),                         (16)
                     *
              =:  JF (&#x03B7;1,&#x03B7;2),                                       (17)
" class="math-display" ></div>
     <!--l. 212--><p class="noindent" ></div>since <span 
class="cmmi-10x-x-109">&#x03B7;</span><sup><span 
class="cmsy-8">*</span></sup> := <img 
src="JensenShannonDiv36x.png" alt="&#x03B7;1+&#x03B7;2
  2"  class="frac" align="middle"> (a right-sided <span 
class="cmti-10x-x-109">Bregman centroid</span>&#x00A0;[<a 
href="#XSBD-2009">13</a>]).
     </li></ul>
<!--l. 1--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-30002"></a>References</h3>
<!--l. 1--><p class="noindent" >
    <div class="thebibliography">
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XVIGJSD-2020"></a>[1] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jacob  Deasy,  Nikola  Simidjievski,  and  Pietro  Liò.   Constraining  Variational  Inference
    with Geometric Jensen-Shannon Divergence.  In <span 
class="cmti-10x-x-109">Advances in Neural Information Processing</span>
    <span 
class="cmti-10x-x-109">Systems</span>, 2020.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XJSmetric-2003"></a>[2] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Dominik&#x00A0;Maria  Endres  and  Johannes&#x00A0;E  Schindelin.    A  new  metric  for  probability
    distributions. <span 
class="cmti-10x-x-109">IEEE Transactions on Information theory</span>, 49(7):1858&#8211;1860, 2003.
                                                                                         
                                                                                         
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XJSmetric-2004"></a>[3] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Bent  Fuglede  and  Flemming  Topsoe.    Jensen-Shannon  divergence  and  Hilbert  space
    embedding. In <span 
class="cmti-10x-x-109">International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings.</span>,
    page&#x00A0;31. IEEE, 2004.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XJeffreys-1946"></a>[4] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Harold  Jeffreys.   An  invariant  form  for  the  prior  probability  in  estimation  problems.
    <span 
class="cmti-10x-x-109">Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences</span>,
    186(1007):453&#8211;461, 1946.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XKullback-1997"></a>[5] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Solomon Kullback. <span 
class="cmti-10x-x-109">Information theory and statistics</span>. Courier Corporation, 1997.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XskewJS-2001"></a>[6] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lillian Lee.  On the effectiveness of the skew divergence for statistical language analysis.
    In <span 
class="cmti-10x-x-109">Artificial Intelligence and Statistics (AISTATS)</span>, page 65&#8211;72, 2001.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XJS-1991"></a>[7] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jianhua Lin. Divergence measures based on the Shannon entropy. <span 
class="cmti-10x-x-109">IEEE Transactions on</span>
    <span 
class="cmti-10x-x-109">Information theory</span>, 37(1):145&#8211;151, 1991.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="XJW-1988"></a>[8] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jianhua Lin and SKM Wong.  Approximation of discrete probability distributions based
    on a new divergence measure. <span 
class="cmti-10x-x-109">Congressus Numerantium (Winnipeg)</span>, 61:75&#8211;80, 1988.
    </p>
    <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xnielsen2010family"></a>[9] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank Nielsen. A family of statistical symmetric divergences based on Jensen&#8217;s inequality.
    <span 
class="cmti-10x-x-109">arXiv preprint arXiv:1009.4004</span>, 2010. URL <a 
href="https://arxiv.org/abs/1009.4004" class="url" ><span 
class="cmtt-10x-x-109">https://arxiv.org/abs/1009.4004</span></a>.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XJSsym-2019"></a>[10] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank  Nielsen.     On  the  Jensen&#8211;Shannon  Symmetrization  of  Distances  Relying  on
    Abstract  Means.   <span 
class="cmti-10x-x-109">Entropy</span>,  21(5),  2019.   ISSN  1099-4300.   doi:  10.3390/e21050485.   URL
    <a 
href="https://www.mdpi.com/1099-4300/21/5/485" class="url" ><span 
class="cmtt-10x-x-109">https://www.mdpi.com/1099-4300/21/5/485</span></a>.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnielsen2020generalization"></a>[11] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank  Nielsen.     On  a  Generalization  of  the  Jensen&#8211;Shannon  Divergence  and  the
    Jensen&#8211;Shannon Centroid.  <span 
class="cmti-10x-x-109">Entropy</span>, 22(2), 2020.  ISSN 1099-4300.  doi: 10.3390/e22020221.
    URL <a 
href="https://www.mdpi.com/1099-4300/22/2/221" class="url" ><span 
class="cmtt-10x-x-109">https://www.mdpi.com/1099-4300/22/2/221</span></a>.
                                                                                         
                                                                                         
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XvJSD-2021"></a>[12] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank Nielsen.  On a Variational Definition for the Jensen-Shannon Symmetrization of
    Distances Based on the Information Radius.  <span 
class="cmti-10x-x-109">Entropy</span>, 23(4), 2021.  ISSN 1099-4300.  doi:
    10.3390/e23040464. URL <a 
href="https://www.mdpi.com/1099-4300/23/4/464" class="url" ><span 
class="cmtt-10x-x-109">https://www.mdpi.com/1099-4300/23/4/464</span></a>.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XSBD-2009"></a>[13] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank  Nielsen  and  Richard  Nock.   Sided  and  symmetrized  Bregman  centroids.   <span 
class="cmti-10x-x-109">IEEE</span>
    <span 
class="cmti-10x-x-109">transactions on Information Theory</span>, 55(6):2882&#8211;2904, 2009.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XCauchyJSD-2021"></a>[14] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frank  Nielsen  and  Kazuki  Okamura.   On  <span 
class="cmmi-10x-x-109">f</span>-divergences  between  cauchy  distributions.
    <span 
class="cmti-10x-x-109">arXiv:2101.12459</span>, 2021.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XRenyi-1961"></a>[15] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alfréd Rényi et&#x00A0;al. On measures of entropy and information. In <span 
class="cmti-10x-x-109">Proceedings of the Fourth</span>
    <span 
class="cmti-10x-x-109">Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to</span>
    <span 
class="cmti-10x-x-109">the Theory of Statistics</span>. The Regents of the University of California, 1961.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XSibson-1969"></a>[16] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robin Sibson. Information radius. <span 
class="cmti-10x-x-109">Zeitschrift f</span><span 
class="cmti-10x-x-109">ür Wahrscheinlichkeitstheorie und verwandte</span>
    <span 
class="cmti-10x-x-109">Gebiete</span>, 14(2):149&#8211;160, 1969.
    </p>
    <p class="bibitem" ><span class="biblabel">
<a 
 id="XWongYOU-1985"></a>[17] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrew&#x00A0;KC  Wong  and  Manlai  You.    Entropy  and  distance  of  random  graphs  with
    application to structural pattern recognition.  <span 
class="cmti-10x-x-109">IEEE Transactions on Pattern Analysis and</span>
    <span 
class="cmti-10x-x-109">Machine Intelligence</span>, (5):599&#8211;609, 1985.
</p>
    </div>
    
</body></html> 

                                                                                         


