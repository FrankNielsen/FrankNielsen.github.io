%\documentclass[11pt]{article}
\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage[numbers]{natbib}
\usepackage{hyperref}

\title{Jensen-Shannon divergence and diversity index:\\ Origins and some extensions}

\author{Frank Nielsen\\ Sony Computer Science Laboratories Inc.\\ Tokyo, Japan}

\date{April 2021}

\def\calE{\mathcal{E}}
\def\calD{\mathcal{D}}
\def\calX{\mathcal{X}}
\def\calF{\mathcal{F}}
\def\calP{\mathcal{P}}
\def\dmu{\mathrm{d}\mu}
\def\KL{\mathrm{KL}}
\def\JS{\mathrm{JS}}
\def\vJS{\mathrm{vJS}}
\def\IR{\mathrm{IR}}

%\def\cite#1{\citep{#1}}

\begin{document}
\maketitle

\begin{abstract}
Lin coined the skewed Jensen-Shannon divergence between two distributions in 1991, and further extended it to the Jensen-Shannon diversity of a set of distributions.
Sibson proposed the information radius based on R\'enyi $\alpha$-entropies in 1969, and recovered for the special case of $\alpha=1$ the Jensen-Shannon diversity index. In this note, we summarize how  the Jensen-Shannon divergence and diversity index were extended by either considering skewing vectors or using mixtures induced by generic means.
\end{abstract}

%%%
\section{Origins}
%%%%%

Let $(\calX,\calF,\mu)$ be a measure space, and $(w_1,P_1),\ldots, (w_n,P_n)$ be $n$ weighted probability measures dominated 
by a measure $\mu$ (with $w_i>0$ and $\sum w_i=1$). 
Denote by $\calP:=\{(w_1,p_1),\ldots,  (w_n,p_n)\}$ the set of their weighted Radon-Nikodym densities $p_i=\frac{\mathrm{d}P_i}{\dmu}$ with respect to $\mu$.

A {\em statistical divergence} $D[p:q]$ is a measure of dissimilarity between two densities $p$ and $q$ (i.e., a $2$-point distance) such that $D[p:q]\geq 0$ with equality if and only if $p(x)=q(x)$ $\mu$-almost everywhere.
A {\em statistical diversity index} $D(\calP)$ is a measure of variation of the weighted densities in $\calP$ related to a measure of centrality, i.e., a $n$-point distance which generalizes the notion of $2$-point distance when $\calP_2(p,q):=\{(\frac{1}{2},p_1),(\frac{1}{2},p_2)\}$:
$$
D[p:q]:=D(\calP_2(p,q)).
$$

The fundamental measure of dissimilarity in information theory is the {\em $I$-divergence} (also called the {\em Kullback-Leibler divergence}, KLD,  see Equation~(2.5) page 5~of~\citep{Kullback-1997}):
$$
D_\KL[p:q]:=  \int_\calX p(x)\log\left(\frac{p(x)}{q(x)}\right)\dmu(x).
$$

The KLD is asymmetric (hence the delimiter notation ``:'' instead of `,') but can be symmetrized by defining the Jeffreys {\em $J$-divergence} (Jeffreys divergence, denoted by $I_2$ in Equation (1) in 1946's paper~\citep{Jeffreys-1946}):
$$
D_J[p,q] := D_\KL[p:q]+D_\KL[q:p] = \int_\calX (p(x)-q(x))\log\left(\frac{p(x)}{q(x)}\right)\dmu(x).
$$
Although symmetric, any positive power of Jeffreys divergence fails to satisfy the triangle inequality: 
That is, $D_J^\alpha$ is never a metric distance for any $\alpha>0$, and furthermore $D_J^\alpha$  cannot be upper bounded.

In 1991, Lin proposed the asymmetric {\em $K$-divergence} (Equation (3.2) in~\citep{JS-1991}):
$$
D_K[p:q]:=D_\KL\left[p:\frac{p+q}{2}\right],
$$
and defined the {\em $L$-divergence} by analogy to Jeffreys's symmetrization of the KLD (Equation (3.4) in~\citep{JS-1991}):
$$
D_L[p,q]=D_K[p:q]+D_K[q:p].
$$

By noticing that 
$$
D_L[p,q]= 2 h\left[\frac{p+q}{2}\right]-(h[p]+h[q]),
$$ 
where $h$ denotes Shannon entropy (Equation (3.14) in~\citep{JS-1991}), Lin coined the (skewed) {\em Jensen-Shannon divergence} between two weighted densities $(1-\alpha,p)$ and $(\alpha,q)$ for $\alpha\in(0,1)$ as follows (Equation (4.1) in~\citep{JS-1991}):
\begin{equation}\label{eq:JSh}
D_{\JS,\alpha}[p,q]=h[(1-\alpha)p+\alpha q]-(1-\alpha)h[p]-\alpha h[q].
\end{equation}

Finally, Lin defined the {\em generalized Jensen-Shannon divergence} (Equation (5.1) in~\citep{JS-1991}) for a finite weighted set of densities:
$$
D_\JS[\calP]=h\left[\sum_i w_ip_i\right]-\sum_i w_i h[p_i].
$$
This generalized Jensen-Shannon divergence is nowadays called the {\em Jensen-Shannon diversity index}.

To contrast with the Jeffreys' divergence, the Jensen-Shannon divergence (JSD) $D_\JS:=D_{\JS,\frac{1}{2}}$ is upper bounded by $\log 2$ (does not require the densities to have the same support), and $\sqrt{D_\JS}$ is 
a metric distance~\citep{JSmetric-2003,JSmetric-2004}.
Lin cited precursor work~\citep{WongYOU-1985,JW-1988} yielding definition of the Jensen-Shannon divergence:
The Jensen-Shannon divergence  of Eq.~\ref{eq:JSh} is the so-called ``increments of entropy'' defined in (19) and (20) of~\citep{WongYOU-1985}.

The Jensen-Shannon diversity index was also obtained very differently by Sibson in 1969 when he defined the {\em information radius}~\citep{Sibson-1969} of order $\alpha$ using R\'enyi $\alpha$-means and R\'enyi $\alpha$-entropies~\citep{Renyi-1961}.
In particular, the information radius $\IR_1$ of order $1$ of a weighted set $\calP$ of densities is a diversity index obtained by solving the following variational optimization problem:
\begin{equation}
\IR_{1}[\calP]:=\min_{c} \sum_{i=1}^n w_i D_\KL[p_i:c].  \label{eq:Sibson}
\end{equation}

Sibson solved a more general optimization problem, and obtained the following expression (term $K_1$ in Corollary 2.3~\citep{Sibson-1969}):
$$
\IR_{1}[\calP]=  h\left[\sum_i w_ip_i\right]-\sum_i w_i h[p_i]:=D_\JS[\calP].
$$
Thus Eq.~\ref{eq:Sibson} is a variational definition of the Jensen-Shannon divergence.

%%%
\section{Some extensions}
%%%%

\begin{itemize}

	\item {\bf Skewing the JSD.} 
	
	The $K$-divergence of Lin can be skewed with a scalar parameter $\alpha\in(0,1)$ to give
	\begin{equation}\label{eq:divK}
	D_{K,\alpha}[p:q]:=D_\KL\left[p:(1-\alpha)p+\alpha q\right].
	\end{equation}
	Skewing parameter $\alpha$ was first studied in~\citep{skewJS-2001} (2001, see Table~2 of~\citep{skewJS-2001}).
	We proposed to unify the Jeffreys divergence with the Jensen-Shannon divergence as follows (Equation 19 in~\citep{nielsen2010family}):
	\begin{equation}\label{eq:JJSalpha}
	D_{K,\alpha}^J[p:q]:=\frac{D_{K,\alpha}[p:q]+D_{K,\alpha}[q:p]}{2}.
	\end{equation}
	When $\alpha=\frac{1}{2}$, we have $D_{K,\frac{1}{2}}^J=D_\JS$, and when $\alpha=1$, we get $D_{K,1}^J=\frac{1}{2}D_J$.
	
	Notice that 
	$$
	D_\JS^{\alpha,\beta}[p;q]:=(1-\beta)D_\KL[p:(1-\alpha)p+\alpha q]+\beta D_\KL[q:(1-\alpha)p+\alpha q]
	$$ 
	amounts to calculate
	 $$
	h^\times[(1-\beta)p+\beta q:(1-\alpha)p+\alpha q]-((1-\beta)h[p]+\beta h[q])
	$$ 
	where 
	$$
	h^\times[p,q]:=\int -p(x)\log q(x)\dmu(x)
	$$ 
	denotes the {\em cross-entropy}. By choosing $\alpha=\beta$, we have $h^\times[(1-\beta)p+\beta q:(1-\alpha)p+\alpha q]=h[(1-\alpha)p+\alpha q]$, 
	and thus recover the skewed Jensen-Shannon divergence of Eq.~\ref{eq:JSh}.
	
	
	In~\citep{nielsen2020generalization} (2020), we considered a positive {\em skewing vector} $\alpha\in [0,1]^k$  and a unit positive weight $w$ belonging to the standard simplex $\Delta_k$, and defined the following {\em vector-skewed Jensen-Shannon divergence}:
\begin{eqnarray}
	D_\JS^{\alpha,w}[p:q] &:=& \sum_{i=1}^k D_\KL[(1-\alpha_i)p_+\alpha_i q : (1-\bar\alpha)p+\bar\alpha q],\\
	&=& h[(1-\bar\alpha)p+\bar\alpha q]-\sum_{i=1}^k h[(1-\alpha_i)p_+\alpha_i q],
\end{eqnarray}
	where $\bar\alpha=\sum_{i=1}^k w_i\alpha_i$. 
	The divergence $D_\JS^{\alpha,w}$ generalizes the (scalar) skew Jensen-Shannon divergence when $k=1$, and is a Ali-Silvey-Csisz\'ar $f$-divergence upper bounded by $\log \frac{1}{\bar\alpha(1-\bar\alpha)}$~\citep{nielsen2020generalization}.
	
	
	\item {\bf A priori mid-density}. The JSD can be interpreted as the total divergence of the densities to the {\em mid-density} $\bar{p}=\sum_{i=1}^n w_i p_i$, a statistical mixture:
	$$
	D_\JS[\calP] = \sum_{i=1}^n w_i D_\KL[p_i:\bar{p}] = h[\bar{p}]-\sum_{i=1}^n w_i h[p_i].
	$$
	Unfortunately, the JSD between two Gaussian densities is not known in closed form because of the definite integral of a log-sum term (i.e., $K$-divergence between a density and a mixture density $\bar{p}$).
	For the special case of the Cauchy family,  a closed-form formula~\citep{CauchyJSD-2021} for the JSD between two Cauchy densities was obtained.
Thus we may choose a {\em geometric mixture distribution}~\citep{JSsym-2019} instead of the ordinary arithmetic mixture $\bar{p}$. More generally, we can choose any weighted mean $M_\alpha$ (say, the geometric mean, or the harmonic mean, or any other power mean) and define a generalization of the $K$-divergence of Equation~\ref{eq:divK}:
	\begin{equation}
	D_K^{M_\alpha}[p:q] := D_K[p:(pq)_{M_\alpha}],
	\end{equation}
	where 
	$$
	(pq)_{M_\alpha}(x):=\frac{M_\alpha(p(x),q(x))}{Z_{M_\alpha}(p:q)}
	$$
	 is a statistical $M$-mixture with $Z_{M_\alpha}(p,q)$
	denoting the normalizing coefficient:
	$$
	Z_{M_\alpha}(p:q)=\int M_\alpha(p(x),q(x))\dmu(x)
	$$ 
	so that $\int (pq)_{M_\alpha}(x)\dmu(x)=1$.
	These $M$-mixtures are well-defined provided the convergence of the definite integrals.
	
	Then we define a generalization of the JSD~\citep{JSsym-2019} termed {\em $(M_\alpha,N_\beta)$-Jensen-Shannon divergence} as follows:
		\begin{equation}
	D_\JS^{M_\alpha,N_\beta}[p:q ]:= N_\beta\left(D_K[p:(pq)_{M_\alpha}] , D_K[q:(pq)_{M_\alpha}]\right),
	\end{equation}
	where $N_\beta$ is yet another weighted mean to average the two $M_\alpha$-$K$-divergences. 
	We have $D_\JS=D_\JS^{A,A}$ where $A(a,b)=\frac{a+b}{2}$ is the arithmetic mean.
	The geometric JSD yields a closed-form formula between two multivariate Gaussians, and has been used in deep learning~\citep{VIGJSD-2020}.
		More generally, we may consider the Jensen-Shannon symmetrization of an arbitrary distance $D$ as  
			\begin{equation}
	D^\JS_{M_\alpha,N_\beta}[p:q]:= N_\beta\left(D[p:(pq)_{M_\alpha}],D[q:(pq)_{M_\alpha}]\right).
	\end{equation}
 %We have the JS-symmetrization of the reverse KLD with respect to the geometric mean which amounts to the skewed Bhattacharyya divergence: $(D_\KL^*)_{G_\alpha}^\JS=D_{\mathrm{Bhat},1-\alpha}$.
	
	\item {\bf A posteriori mid-density}.
	We consider a generalization of Sibson's information radius~\citep{Sibson-1969}.
	Let $S_w(a_1,\ldots,a_n)$ denote a generic weighted mean of $n$ positive scalars $a_1,\ldots, a_n$, with weight vector $w\in\Delta_n$.
	Then we define the {\em $S$-variational Jensen-Shannon diversity index}~\citep{vJSD-2021} as
	\begin{equation}
	D_\vJS^{S_w}(\calP) := \min_{c} S_w\left(D_\KL[p_1:c],D_\KL[p_n:c]\right).
	\end{equation}
	When $S_w=A_w$ (with $A_w(a_1,\ldots,a_n)=\sum_{i=1}^n w_i a_i$ the arithmetic weighted mean), we recover the ordinary Jensen-Shannon diversity index.
			More generally, we define the {\em $S$-Jensen-Shannon index of an arbitrary distance $D$} as
	\begin{equation}
D^\vJS_{S_w}(\calP):=\min_{c} S_w\left(D[p_1:c],\ldots, D[p_n:c]\right).	
\end{equation}
When $n=2$, this yields a Jensen-Shannon-symmetrization of distance $D$.

The variational optimization defining the JSD can also be constrained to a (parametric) family of densities $\calD$, thus defining 
	the {\em $(S,\calD)$-relative Jensen-Shannon diversity index}:
	\begin{equation}
	D_\vJS^{S_w,\calD}(\calP) := \min_{c\in\calD} S_w\left(D_\KL[p_1:c],\ldots, D_\KL[p_n:c]\right).
	\end{equation}
	

The relative Jensen-Shannon divergences are useful for clustering applications:
Let $p_{\theta_1}$ and $p_{\theta_2}$ be two densities of an exponential family $\mathcal{E}$ with cumulant function $F(\theta)$.
Then the $\mathcal{E}$-relative Jensen-Shannon divergence is the Bregman information of $\calP_2(p,q)$ for the conjugate function $F^*(\eta)=-h[p_\theta]$ 
(with $\eta=\nabla F(\theta)$). The $\calE$-relative JSD amounts to  a {\em Jensen divergence} for $F^*$:

\begin{eqnarray}
D_\vJS[p_{\theta_1},p_{\theta_2}] &=& \min_\theta \frac{1}{2}\left\{D_\KL[p_{\theta_1}:p_{\theta}]+D_\KL[p_{\theta_2}:p_{\theta}]\right\},\\
 &=& \min_\theta \frac{1}{2}\left\{B_F[\theta:\theta_1]+B_F[\theta:\theta_2]\right\},\\
 &=&  \min_\eta  \frac{1}{2}\left\{B_{F^*}[\eta_1:\eta]+B_{F^*}[\eta_2:\eta]\right\},\\
&=& \frac{F^*(\eta_1)+F^*(\eta_2)}{2}-F^*(\eta^*),\\
&=:& J_{F^*}(\eta_1,\eta_2),
\end{eqnarray}
since $\eta^*:=\frac{\eta_1+\eta_2}{2}$ (a right-sided {\em Bregman centroid}~\cite{SBD-2009}).

%For example, for the normal family $\mathcal{N}$, we have $F^*(\eta)=\frac{1}{2}\log(\eta_2-\eta_1^2)+\frac{1}{2}\log(2\pi e)$ with $\eta=(\mu,\mu^2+\sigma^2)$.
%We have $\eta^*=\left(\frac{\mu_1+\mu_2}{2},\frac{\mu_1^2+\sigma_1^2+\mu_2^2+\sigma_2^2}{2}\right)$.


	



\end{itemize}

 
 

%\bibliographystyle{plain}
\bibliographystyle{plainnat}
\bibliography{JSDBIB}
\end{document}