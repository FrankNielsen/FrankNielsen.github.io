<html>
<head>
<title>Joint Structures and Common Foundations of Statistical Physics,
Information Geometry and Inference for Learning (SP+IG'20 @ Les Houches, 26-30th July 2020)</title>
</head>

<body>
 <center>
<img src="SPIG20-Sponsors.png" width="70%">
</center>

<h1>
Joint Structures and Common Foundations of Statistical Physics,
Information Geometry and Inference for Learning (SP+IG'20)
</h1>

<h2>
Date: 26th July to 31st July 2020
</h2>

Location: <A HREF="https://houches.univ-grenoble-alpes.fr/" target=”_blank”>Ecole de Physique des Houches</A><BR>
<code>
Ecole de Physique des Houches<BR>
https://houches.univ-grenoble-alpes.fr/<BR>
149 Chemin de la Côte, F-74310 Les Houches, France<BR>
(+33/0) 4 57 04 10 40<BR>
</code>

<p>
Download the poster in <A HREF="Les-houches_Workshop_proposal _Physique Statistique et Géométrie de l'Information-EN-revA.pdf" target=”_blank”>pdf</A>
</p>

<p>
To submit a short paper or poster, please use the Easychair conference system:<BR>
<blink><A HREF="https://easychair.org/conferences/?conf=spig20" target=”_blank”>https://easychair.org/conferences/?conf=spig20</A>
</blink>
</p>


<center>
<img src="Affiche Houches.png" width="33%">
</center>

<h2>Scientific rationale:</h2>

<p>
In the middle of the last century, Léon Brillouin in "The Science and The Theory of
Information" or André Blanc-Lapierre in "Statistical Mechanics" forged the first links
between the Theory of Information and Statistical Physics as precursors.
In the context of Artificial Intelligence, machine learning algorithms use more and
more methodological tools coming from the Physics or the Statistical Mechanics. The
laws and principles that underpin this Physics can shed new light on the conceptual
basis of Artificial Intelligence. Thus, the principles of Maximum Entropy, Minimum of
Free Energy, Gibbs-Duhem's Thermodynamic Potentials and the generalization of
François Massieu's notions of characteristic functions enrich the variational
formalism of machine learning. Conversely, the pitfalls encountered by Artificial
Intelligence to extend its application domains, question the foundations of Statistical
Physics, such as the construction of stochastic gradient in large dimension, the
generalization of the notions of Gibbs densities in spaces of more elaborate
representation like data on homogeneous differential or symplectic manifolds, Lie
groups, graphs, tensors, ....
Sophisticated statistical models were introduced very early to deal with
unsupervised learning tasks related to Ising-Potts models (the Ising-Potts model
defines the interaction of spins arranged on a graph) of Statistical Physics. and
more generally the Markov fields. The Ising models are associated with the theory
of Mean Fields (study of systems with complex interactions through simplified
models in which the action of the complete network on an actor is summarized by a
single mean interaction in the sense of the mean field).
The porosity between the two disciplines has been established since the birth of
Artificial Intelligence with the use of Boltzmann machines and the problem of robust
methods for calculating partition function. More recently, gradient algorithms for
neural network learning use large-scale robust extensions of the natural gradient of
Fisher-based Information Geometry (to ensure reparameterization invariance), and
stochastic gradient based on the Langevin equation (to ensure regularization), or
their coupling called "Natural Langevin Dynamics".
Concomitantly, during the last fifty years, Statistical Physics has been the object
of new geometrical formalizations (contact or symplectic geometry, ...) to try to
give a new covariant formalization to the thermodynamics of dynamic systems. We
can mention the extension of the symplectic models of Geometric Mechanics to
Statistical Mechanics, or other developments such as Random Mechanics, Geometric
Mechanics in its Stochastic version, Lie Groups Thermodynamic, and geometric
modeling of phase transition phenomena.
Finally, we refer to Computational Statistical Physics, which uses efficient
numerical methods for large-scale sampling and multimodal probability
measurements (sampling of Boltzmann-Gibbs measurements and calculations of
free energy, metastable dynamics and rare events, ...) and the study of geometric
integrators (Hamiltonian dynamics, symplectic integrators, ...) with good properties
of covariances and stability (use of symmetries, preservation of invariants, ...).
Machine learning inference processes are just beginning to adapt these new
integration schemes and their remarkable stability properties to increasingly
abstract data representation spaces.
Artificial Intelligence currently uses only a very limited portion of the conceptual
and methodological tools of Statistical Physics. The purpose of this conference is to
encourage constructive dialogue around a common foundation, to allow the
establishment of new principles and laws governing the two disciplines in a unified
approach. But, it is also about exploring new « chemins de traverse ».
</p>

<h2>Organizers:</h2>
<ul>
 <li>Frédéric Barbaresco, THALES, KTD PCC, Palaiseau, France
  <li>Silvère Bonnabel, Mines ParisTech, CAOR, Paris, France
   <li>François Gay-Balmaz, Ecole Normale Supérieure Ulm, CNRS & LMD, Paris, France
    <li>Patrick Iglesias-Zemmour, Université de Marseille, I2M, Marseille, France
   <li>Bernhard Maschke, Université Claude Bernard, LAGEPP, Lyon, France
 <li>Eric Moulines, Ecole Polytechnique, CMAP, Palaiseau, France
 <li>Frank Nielsen, Sony Computer Science Laboratories, Tokyo, Japan and Ecole Polytechnique, France
 <li>Gery de Saxcé, Université de Lille, LAM3, Lille, France
</ul>

<hr>

<center>
<img src="LesHouchesLocation.png" width="50%">
</center>

<hr>
Last updated, Nov. 17 2019 
</body>

</html>
