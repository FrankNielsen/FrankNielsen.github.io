<h1>Ten great articles in information geometry</h1>

Disclaimer: The list below is a personal selection of historical articles which is highly suggestive 
as so many good papers have been published in information geometry.
Yet, I think reading/browsing those 10 papers could be useful for researchers/engineers starting in information geometry.
The papers are not listed in chronological order.

<ol>

<li><A HREF="https://www.tandfonline.com/doi/pdf/10.1080/02331887808801428">Algebraic foundation 
of mathematical statistics</A>, Nikolai Nikolaevich Čencov (Chentsov), 
Statistics: A Journal of Theoretical and Applied Statistics 9.2 (1978): 267-276.

<p style="background-color: #EDF7FF;">A very readable synthesis of Chentsov monograph results defining statistical invariance, total variation, Kullback-Leibler, and Chernoff divergences, etc</p>

<li><A HREF="https://projecteuclid.org/journals/annals-of-statistics/volume-3/issue-6/Defining-the-Curvature-of-a-Statistical-Problem-with-Applications-to/10.1214/aos/1176343282.full">
Defining the Curvature of a Statistical Problem (with Applications to Second Order Efficiency)</A>, Bradley Efron, 
 The Annals of Statistics (1975): 1189-1242.
 
 
 <p style="background-color: #EDF7FF;">Statistical inference on curved exponential families linked with a novel notion of statistical curvature, information loss, etc. Great discussions of the paper by many Statisticians.</p>



 <li>PDF <A HREF="https://www.fuw.edu.pl/~kostecki/scans/nagaokaamari1982.pdf">Differential geometry 
 of smooth families of probability distributions</A>,  Hiroshi Nagaoka and Shun-ichi Amari,
METR (1982): 82-7.

 
 <p style="background-color: #EDF7FF;">New theory of dual connections and key theorems (Pythagoras theorem, projections, etc).
 </p>


<li><A HREF="https://ieeexplore.ieee.org/document/6790500">Natural gradient works efficiently in learning</A>, Shun-ichi Amari, Neural computation 10.2 (1998): 251-276.
 <p style="background-color: #EDF7FF;">Introduce natural gradient on Riemannian manifolds and demonstrate theoretically its efficiency.
 </p>

<li><A HREF="https://projecteuclid.org/journals/annals-of-statistics/volume-11/issue-3/Second-Order-Efficiency-of-Minimum-Contrast-Estimators-in-a-Curved/10.1214/aos/1176346246.full">Second order efficiency of minimum contrast estimators in a curved exponential family</A>, 
Shinto Eguchi, The Annals of Statistics (1983): 793-803.

 <p style="background-color: #EDF7FF;">Introduce information geometry of constrast functions/divergences.
 </p>
 
<li><A HREF="https://link.springer.com/article/10.1023/A:1003569210573">A characterization of monotone and regular divergences</A>, 
José Manuel Corcuera  and Federica Giummole,
Annals of the Institute of Statistical Mathematics 50 (1998): 433-450.

 <p style="background-color: #EDF7FF;">Characterize monotone divergences and regular divergences. Taylor expansions.
 </p>
 
 <li>PDF <A HREF="https://www.jmlr.org/papers/volume4/cardoso03a/cardoso03a.pdf">Dependence, 
 correlation and gaussianity in independent component analysis</A>, 
 Jean-François Cardoso, The Journal of Machine Learning Research 4 (2003): 1177-1203.
 
  <p style="background-color: #EDF7FF;">Information geometry in action for ICA
 </p>

<li><A HREF="https://ieeexplore.ieee.org/document/930911">Information geometry on hierarchy of probability distributions</A>, Shun-ichi Amari, IEEE transactions on information theory 47.5 (2001): 1701-1711.

 <p style="background-color: #EDF7FF;">Mixed primal/dual coordinate systems, dual foliations, and divergence decomposition.
 </p>
 
 
 <li>PDF <A HREF="https://bsi-ni.brain.riken.jp/database/file/136/134.pdf">Information geometry of Boltzmann machines</A>,
 Shun-ichi Amari, Koji Kurata and Hiroshi Nagaoka, IEEE Transactions on neural networks 3.2 (1992): 260-271.

 <p style="background-color: #EDF7FF;">Information geometry in action for neural networks</p>
 
<li>PDF <A HREF="https://projecteuclid.org/journals/annals-of-probability/volume-46/issue-2/Exponentially-concave-functions-and-a-new-information-geometry/10.1214/17-AOP1201.pdf">Exponentially concave functions and a new information geometry</A>,
Soumik Pal,  and Ting-Kam Leonard Wong, The Annals of probability 46.2 (2018): 1070-1113.

 <p style="background-color: #EDF7FF;">Logarithmic divergences extends Bregman divergences and are canonical divergences in constant sectional curvature manifolds</p>
 
</ol>
 
 <!--
Kurose, Takashi. "On the divergences of 1-conformally flat statistical manifolds." Tohoku Mathematical Journal, Second Series 46.3 (1994): 427-433.

Murata, N., Takenouchi, T., Kanamori, T., & Eguchi, S. (2004). Information geometry of U-Boost and Bregman divergence. Neural Computation, 16(7), 1437-1481. -->